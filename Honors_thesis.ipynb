{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Honors_thesis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "24aVbFf1qrK8",
        "E0edb2vY2bd4"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kfin2343/riboswitches/blob/main/Honors_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAwxIBBwBkdk"
      },
      "source": [
        "#function setup & data pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akVCi9MXL_fs",
        "outputId": "ea638803-8d4b-4b09-8d85-054165e2711c"
      },
      "source": [
        "!pip install biopython"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.6/dist-packages (1.78)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjW7iDA_BhQE"
      },
      "source": [
        "import os, requests, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.cm as cm\n",
        "import statistics\n",
        "from statistics import mean\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from Bio import Align\n",
        "from Bio.Align import substitution_matrices\n",
        "from Bio import pairwise2"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izC3ae4qBpbH"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLWwtHE6_iiV",
        "outputId": "69cc36b6-0ba8-4784-e17b-7438ac78794b"
      },
      "source": [
        "##### REMOVE OUTLIERS BY TAKING SWITCHES OF LENGTH 50 TO 110 #######\n",
        "num =  80\n",
        "var = 50\n",
        "#### IMPORT FAMILIES \n",
        "#sets = ['Fluoride', 'Glutamine', 'Glycine', 'Purine', 'ZMP']\n",
        "#sets = ['Fluoride', 'Glutamine', 'Glycine', 'Purine', 'ZMP', 'FMN', 'SAMIIV', 'TPP', 'nhaA']\n",
        "#sets = ['Glutamine', 'Glycine', 'ZMP']\n",
        "sets = ['Glycine']\n",
        "num_classes = 1\n",
        "fam_len = []\n",
        "fam_sequences = []\n",
        "lll = []\n",
        "for u in sets:\n",
        "  url = 'https://raw.githubusercontent.com/Kfin2343/riboswitches/main/{}_riboswitch.csv'.format(u)\n",
        "  s=requests.get(url).content\n",
        "  df=pd.read_csv(io.StringIO(s.decode('utf-8')), sep = \",\") #pandas datatframe\n",
        "  sequence1 = np.asarray(df['0'])\n",
        "  exact_len = []\n",
        "  ll = []\n",
        "  for i in range(len(sequence1)): \n",
        "    l = len(sequence1[i])\n",
        "    ll.append(l)\n",
        "    if num-var< l < num+var: #only accept lengths between 50 and 110\n",
        "      a = sequence1[i]\n",
        "      exact_len.append(a)\n",
        "  sequence = np.asarray(exact_len) #convert to numpy array\n",
        "  sequence = np.expand_dims(sequence, 1) #2d array\n",
        "  fam_len.append(len(sequence))  #class size, used for label creation\n",
        "  fam_sequences.append(sequence) \n",
        "  lll.append(mean(ll))\n",
        "ex_seq = fam_sequences[0] #needed for concatenating along new dim (below)\n",
        "for i in range(1,len(fam_len)):\n",
        "  ex_seq = np.concatenate((ex_seq, fam_sequences[i]))\n",
        "ex_seq = np.squeeze(ex_seq)\n",
        "\n",
        "###create classification label vector\n",
        "fam_idx = [] \n",
        "for i in range(1, len(fam_len)+1):\n",
        "  a = np.empty((fam_len[i-1], 1))\n",
        "  a.fill(i)\n",
        "  fam_idx.append(a)\n",
        "fam_id = np.concatenate((fam_idx), axis = 0)\n",
        "\n",
        "###convert label to one hot encoded vector\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(fam_id) \n",
        "family_id = enc.transform(fam_id).toarray()\n",
        "family_id = torch.tensor(family_id).double().to(device)\n",
        "\n",
        "print(lll)\n",
        "for i in range(num_classes):\n",
        "  print(len(fam_sequences[i])) #family counts"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[95.50765637165844]\n",
            "3853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gX5wCdwBry5"
      },
      "source": [
        "def sequence_one_hot(sequence_array):\n",
        " \n",
        " \n",
        "  seq_list = []\n",
        "  \n",
        "  for a in sequence_array:\n",
        "    split = []\n",
        "    split = [char for char in a]\n",
        "    seq_list.append(split)\n",
        "\n",
        "  resnames = ['G', 'T', 'A', 'C']\n",
        "    \n",
        "  label_encoder = LabelEncoder()\n",
        "  encode_names = label_encoder.fit_transform(resnames)\n",
        "  code = dict(zip(resnames, encode_names))\n",
        "\n",
        "  sequences = []\n",
        "  for i in range(len(seq_list)):\n",
        "    position_sequence = []\n",
        "    for r in seq_list[i]:\n",
        "      position = code[r]\n",
        "      position_sequence.append(position)\n",
        "    \n",
        "    values = np.asarray(position_sequence).reshape(len(position_sequence), 1)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False) #build encoder\n",
        "    enc = encode_names.reshape(len(encode_names),1) #reshape into two dimensions before fitting\n",
        "    onehot_encoder.fit(enc)  #assign constant 'code' for one hot encoding from 'resnames' list above\n",
        "    encoded_matrix = onehot_encoder.transform(values) #convert to [x,20] dimensions, one hot encoded\n",
        "    sequences.append(encoded_matrix)\n",
        "\n",
        "  lx = []\n",
        "  lenx = []\n",
        "\n",
        "  for a in sequences: \n",
        "      tensor = torch.tensor(a) #convert to tensor\n",
        "      lx.append(tensor) #append to list\n",
        "      lenx.append(a.shape[0]) #need length for pack_padded_sequence\n",
        "\n",
        "  padded_sequence = pad_sequence(lx, padding_value=0)\n",
        "\n",
        "  return padded_sequence, lenx\n",
        "\n",
        "padded_sequence, lengths = sequence_one_hot(sequence_array = ex_seq) #processed data\n",
        "padded_sequence = padded_sequence.double() #dtype\n",
        "padded_sequence = padded_sequence.to(device) #cast to gpu\n",
        "lengths = torch.tensor(lengths).double().to(device)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvCuu5wtpDY"
      },
      "source": [
        "def find_similar(seq):\n",
        "  similarity_list = []\n",
        "  seqsplit = split(seq)\n",
        "  for ii in range(len(ex_seq)):  \n",
        "    iq = ex_seq[ii]\n",
        "    gensplit = split(iq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "    a = tally/len(seqsplit)\n",
        "    similarity_list.append(a)\n",
        "\n",
        "  return max(similarity_list)\n",
        "\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wZb1V_QEuXZ"
      },
      "source": [
        "\n",
        "class dataset_rnn(data.Dataset):\n",
        "  def __init__(self, x, y, l):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.l = l\n",
        "  def __len__(self):\n",
        "    return len(self.x[0,:,0])\n",
        "  def __getitem__(self, index):\n",
        "    X = self.x[:, index, :]\n",
        "    Y = self.y[index,:]\n",
        "    L = self.l[index]\n",
        "    return X, Y, L"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28bIumCHEyxT"
      },
      "source": [
        "class dataset(data.Dataset):\n",
        "  def __init__(self, x, t, l):\n",
        "    self.x = x\n",
        "    self.t = t\n",
        "    self.l = l\n",
        "  def __len__(self):\n",
        "    return len(self.x[0,:,0])\n",
        "  def __getitem__(self, index):\n",
        "    X = self.x[:, index, :]\n",
        "    T = self.t[index, :]\n",
        "    L = self.l[index]\n",
        "    return X, T, L"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlv2GXu8cMey",
        "outputId": "151dce5f-43b4-4d47-e85d-7965d9aeb37d"
      },
      "source": [
        "all_index = np.arange(0, len(family_id[:,0]))\n",
        "train_index = []\n",
        "fcs = np.cumsum(fam_len)\n",
        "for i in range(num_classes):\n",
        "  if i == 0:\n",
        "    ix = random.sample(range(0, fcs[0]), 9*len(fam_sequences[i])//10)\n",
        "    train_index.append(ix)\n",
        "  else:\n",
        "    ix = random.sample(range(fcs[i-1], fcs[i]), 9*len(fam_sequences[i])//10)\n",
        "    train_index.append(ix)\n",
        "    \n",
        "a = []\n",
        "for i in train_index:\n",
        "  for x in i:\n",
        "    a.append(x)\n",
        "\n",
        "train_index = np.asarray(a).flatten()\n",
        "rng = np.random.default_rng()\n",
        "rng.shuffle(train_index)\n",
        "print(train_index.shape)\n",
        "test_index = np.delete(all_index, train_index)\n",
        "\n",
        "\n",
        "train_index = torch.tensor(train_index).to(device)\n",
        "valid_index = torch.tensor(test_index).to(device)\n",
        "\n",
        "print(train_index.shape, test_index.shape)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3467,)\n",
            "torch.Size([3467]) (386,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kA2AQK70nJn"
      },
      "source": [
        "train_x = padded_sequence[:,train_index,:]\n",
        "train_length = lengths[train_index]\n",
        "test_length = lengths[valid_index]\n",
        "test_x = padded_sequence[:,valid_index,:]\n",
        "train_y = family_id[train_index,:]\n",
        "test_y = family_id[valid_index,:]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPGZe7H4vpWE"
      },
      "source": [
        "def generate_report(predicted, ground_truth):\n",
        "\n",
        "  predicted = predicted.reshape(predicted.size(0), num_classes).to('cpu')\n",
        "  predicted = np.asarray(predicted.detach())\n",
        "  pred_binary = np.zeros_like(predicted)\n",
        "  pred_binary[np.arange(len(predicted)), predicted.argmax(1)] = 1\n",
        "  ground_truth = np.asarray(ground_truth.to('cpu'))\n",
        "  matrix = classification_report(ground_truth, pred_binary, digits=4)\n",
        "  f1 = f1_score(ground_truth, pred_binary, average='macro')\n",
        "  targets = []\n",
        "  for i in range (num_classes):\n",
        "    a = str(i)\n",
        "    targets.append(a)\n",
        "  ##targets = ['1', '2', '3', '4', '5']\n",
        "  colors = cm.rainbow(np.linspace(0, 1, len(targets)))\n",
        "  plt.figure()\n",
        "  lw = 3\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "\n",
        "  for i, target in enumerate(targets):\n",
        "    fpr[i], tpr[i], thresholds = roc_curve(ground_truth[:,i], predicted[:,i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                        ''.format(i, roc_auc[i]))  # roc_auc_score\n",
        "\n",
        "  avg_score = mean(roc_auc[k] for k in roc_auc)\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=(1.1,0))\n",
        "  #plt.show()\n",
        "\n",
        "  return plt, matrix, avg_score, f1\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB3tW-WXyWDV"
      },
      "source": [
        "def split(word): \n",
        "    return list(word)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5UOZkeAsrol"
      },
      "source": [
        "def process_generated(gen):\n",
        "  test = gen.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "  test = np.asarray(test.detach())\n",
        "\n",
        "  pred_binary = np.zeros_like(test)\n",
        "  pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "  a = np.sum(test, 1)\n",
        "\n",
        "  #plt.plot(a)\n",
        "  #plt.show()\n",
        "\n",
        "  stop_point = []\n",
        "  for i, j in enumerate(a):\n",
        "    if j <=.2:\n",
        "      stop_point.append(i)\n",
        "\n",
        "  #print(stop_point[0:4])\n",
        "  if not stop_point:\n",
        "    pass\n",
        "  elif stop_point[0] <= 40:\n",
        "    pass\n",
        "  else:\n",
        "    pred_binary = pred_binary[:stop_point[0],:]\n",
        "\n",
        "  s = []\n",
        "  for i in pred_binary:\n",
        "    if i[0] == 1:\n",
        "      s.append('A')\n",
        "    elif i[1] == 1:\n",
        "      s.append('C')\n",
        "    elif i[2] == 1:\n",
        "      s.append('G')\n",
        "    elif i[3] == 1:\n",
        "      s.append('T')\n",
        "\n",
        "  result = ''\n",
        "  for element in s:\n",
        "      result += str(element)\n",
        "\n",
        "  generated = [result]\n",
        "\n",
        "  seq, _ = sequence_one_hot(generated)\n",
        "  #print(seq.size(0))\n",
        "  zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "  generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "  g = generated_sequence.permute(1,0,2)\n",
        "  g = g.to(device)\n",
        "  \n",
        "  return result, g\n"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGC0589lJZvn"
      },
      "source": [
        "def cyclic_anneal(epoch, epochs, num_cycles):\n",
        "  cycle_length = epochs//num_cycles\n",
        "  #seperate first x cycles and last cycle\n",
        "  cycle_list = []\n",
        "  for i in range(1,num_cycles+1):\n",
        "    if i == num_cycles:\n",
        "      cycle_list.append(epochs)\n",
        "    else:\n",
        "      a = cycle_length*i\n",
        "      cycle_list.append(a)\n",
        "\n",
        "  if epoch <= cycle_list[0]:\n",
        "    if epoch < cycle_length //2:\n",
        "      beta = epoch/(cycle_length//2)\n",
        "    else:\n",
        "      beta = 1\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  for ii in cycle_list:\n",
        "    r = epoch/ii\n",
        "    if 1<r<=2:\n",
        "      a = epoch-ii\n",
        "      if a < cycle_length //2:\n",
        "        beta = a/(cycle_length//2)\n",
        "      else:\n",
        "        beta = 1\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return beta\n",
        "\n",
        "for i in range(100):\n",
        "  a = cyclic_anneal(i, 100, 3)\n",
        "\n",
        "  "
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfsYCIhJEFG"
      },
      "source": [
        "# vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fxqQfZRy1_"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "midiuuF7GYFg"
      },
      "source": [
        " class VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, hid2_dims=8, num_classes=num_classes, negative_slope=0.1):\n",
        "        super(VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.hid2_dims = hid2_dims\n",
        "        self.num_classes = num_classes\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dims+num_classes, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Conditioner\n",
        "        self.conditioner = nn.Sequential(\n",
        "            nn.Linear(num_classes, 16),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(16,32),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(32, hid2_dims),\n",
        "            nn.ReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hid1_dims+num_classes, 128), \n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 512), \n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x,y], dim=1)\n",
        "        # Encode input\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        # Encode label\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(h)\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hy = self.conditioner(y)\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(h)\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(h)\n",
        "      return(z)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      x = torch.cat([x,y], dim=1)\n",
        "      l1 = self.encoder(x)\n",
        "      mu, logvar = self.fc_mu(l1), self.fc_var(l1)\n",
        "      l2 = self._reparameterize(mu, logvar)\n",
        "      return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "        x = torch.cat([x,y], dim=1)\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8_2fkAWR4pH"
      },
      "source": [
        "##VAE loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMLa_g__Jnr6"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def vae_loss_function(recon_x, x, mu, log_var, epoch,cycles):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = (-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())-5)*cyclic_anneal(epoch, vae_epochs, cycles) \n",
        "    return BCE + 5*KLD, KLD/(KLD+BCE)  #see Serena Yeung et. al. "
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQLgBcZR9O3"
      },
      "source": [
        "##train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ll52wVUbPP"
      },
      "source": [
        "\n",
        "def train_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  vae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss, KL = vae_loss_function(recon_batch, x, mu, log_var, epoch,cycles)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(vae.parameters(), 4)\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)), 'KL:', KL.item())"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vjsBG_T_Zqp"
      },
      "source": [
        "def test_vae(epoch, loader):\n",
        "  test_load = loader\n",
        "  vae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(test_load):\n",
        "      x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTVO-LIkSAEx"
      },
      "source": [
        "##hyperparam, build, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-twHyYyN63e"
      },
      "source": [
        "batch_size_vae = 100\n",
        "vae_epochs = 100\n",
        "latent_size = 16\n",
        "conditional_size = 32 ##ignore\n",
        "cycles = 4"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3vGo4HkJpRr"
      },
      "source": [
        "# build model\n",
        "vae = VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, hid2_dims = conditional_size, num_classes=num_classes, negative_slope=.01).double()\n",
        "vae = vae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(vae.parameters(), lr = .001)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfJAhQTQEih7"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy_NzpjXK-fd",
        "outputId": "5957f3d5-929f-447d-9c60-60b1cb568999"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_vae(epoch, vae_test_loader)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 0 Average loss: 209.5800 KL: 0.0\n",
            "====> test loss: 189.2782\n",
            "====> Epoch: 1 Average loss: 185.8593 KL: 0.0039037912932388245\n",
            "====> test loss: 178.2732\n",
            "====> Epoch: 2 Average loss: 181.5972 KL: 0.006311909304083413\n",
            "====> test loss: 174.1573\n",
            "====> Epoch: 3 Average loss: 179.7003 KL: 0.00815982019623588\n",
            "====> test loss: 170.7713\n",
            "====> Epoch: 4 Average loss: 178.0824 KL: 0.010480999604886005\n",
            "====> test loss: 169.0128\n",
            "====> Epoch: 5 Average loss: 177.3114 KL: 0.012707793075703572\n",
            "====> test loss: 165.8531\n",
            "====> Epoch: 6 Average loss: 177.1960 KL: 0.015026491890019204\n",
            "====> test loss: 164.4077\n",
            "====> Epoch: 7 Average loss: 177.2303 KL: 0.01701149975719807\n",
            "====> test loss: 164.0152\n",
            "====> Epoch: 8 Average loss: 177.7188 KL: 0.018864915717590573\n",
            "====> test loss: 162.9447\n",
            "====> Epoch: 9 Average loss: 178.7724 KL: 0.021095214129662587\n",
            "====> test loss: 161.7223\n",
            "====> Epoch: 10 Average loss: 179.5039 KL: 0.022088455808088464\n",
            "====> test loss: 163.4658\n",
            "====> Epoch: 11 Average loss: 180.3531 KL: 0.0249798045750584\n",
            "====> test loss: 161.8702\n",
            "====> Epoch: 12 Average loss: 181.4637 KL: 0.026028008742600188\n",
            "====> test loss: 160.4118\n",
            "====> Epoch: 13 Average loss: 180.9638 KL: 0.02698119628197188\n",
            "====> test loss: 157.7131\n",
            "====> Epoch: 14 Average loss: 180.6082 KL: 0.02801437492230554\n",
            "====> test loss: 158.6766\n",
            "====> Epoch: 15 Average loss: 179.7784 KL: 0.026997166539746712\n",
            "====> test loss: 160.9395\n",
            "====> Epoch: 16 Average loss: 179.4939 KL: 0.02773552603799215\n",
            "====> test loss: 158.9422\n",
            "====> Epoch: 17 Average loss: 178.8037 KL: 0.02762543022835242\n",
            "====> test loss: 159.5407\n",
            "====> Epoch: 18 Average loss: 177.9432 KL: 0.028292237621657758\n",
            "====> test loss: 158.4479\n",
            "====> Epoch: 19 Average loss: 178.0636 KL: 0.028028175849875516\n",
            "====> test loss: 157.5050\n",
            "====> Epoch: 20 Average loss: 177.3718 KL: 0.029343519332588944\n",
            "====> test loss: 157.2709\n",
            "====> Epoch: 21 Average loss: 176.9590 KL: 0.027946425919412828\n",
            "====> test loss: 155.4903\n",
            "====> Epoch: 22 Average loss: 176.5323 KL: 0.03041125054907838\n",
            "====> test loss: 159.5557\n",
            "====> Epoch: 23 Average loss: 176.2844 KL: 0.030458413732429038\n",
            "====> test loss: 156.6218\n",
            "====> Epoch: 24 Average loss: 175.5987 KL: 0.031243846935041796\n",
            "====> test loss: 154.6977\n",
            "====> Epoch: 25 Average loss: 174.8601 KL: 0.03130745042563197\n",
            "====> test loss: 154.9868\n",
            "====> Epoch: 26 Average loss: 148.0969 KL: 0.00608387687884446\n",
            "====> test loss: 145.6790\n",
            "====> Epoch: 27 Average loss: 148.7204 KL: 0.010229423350528817\n",
            "====> test loss: 144.4704\n",
            "====> Epoch: 28 Average loss: 149.5846 KL: 0.014766994774561193\n",
            "====> test loss: 143.5930\n",
            "====> Epoch: 29 Average loss: 151.3565 KL: 0.018378958980035148\n",
            "====> test loss: 145.8903\n",
            "====> Epoch: 30 Average loss: 153.4614 KL: 0.022593831688627913\n",
            "====> test loss: 143.1452\n",
            "====> Epoch: 31 Average loss: 155.4963 KL: 0.026143695043649735\n",
            "====> test loss: 142.5615\n",
            "====> Epoch: 32 Average loss: 157.4873 KL: 0.027932740591922824\n",
            "====> test loss: 142.9503\n",
            "====> Epoch: 33 Average loss: 159.8304 KL: 0.031217445801624905\n",
            "====> test loss: 143.4833\n",
            "====> Epoch: 34 Average loss: 162.0139 KL: 0.03476021472637575\n",
            "====> test loss: 144.4934\n",
            "====> Epoch: 35 Average loss: 164.1174 KL: 0.03739949758412954\n",
            "====> test loss: 145.1980\n",
            "====> Epoch: 36 Average loss: 166.0799 KL: 0.03949660211263855\n",
            "====> test loss: 144.4382\n",
            "====> Epoch: 37 Average loss: 168.3600 KL: 0.04183794436924591\n",
            "====> test loss: 144.5097\n",
            "====> Epoch: 38 Average loss: 168.3220 KL: 0.04082966883790521\n",
            "====> test loss: 146.3579\n",
            "====> Epoch: 39 Average loss: 168.2380 KL: 0.043379971584556025\n",
            "====> test loss: 145.9989\n",
            "====> Epoch: 40 Average loss: 167.4595 KL: 0.04193348940605663\n",
            "====> test loss: 144.5605\n",
            "====> Epoch: 41 Average loss: 167.1242 KL: 0.04163355704701757\n",
            "====> test loss: 144.9834\n",
            "====> Epoch: 42 Average loss: 166.8413 KL: 0.04268724920415601\n",
            "====> test loss: 146.3037\n",
            "====> Epoch: 43 Average loss: 166.9536 KL: 0.042205253107109826\n",
            "====> test loss: 145.3019\n",
            "====> Epoch: 44 Average loss: 166.5761 KL: 0.04308840681815655\n",
            "====> test loss: 146.1665\n",
            "====> Epoch: 45 Average loss: 166.1172 KL: 0.044316357738469966\n",
            "====> test loss: 144.4917\n",
            "====> Epoch: 46 Average loss: 165.7531 KL: 0.04211438964160761\n",
            "====> test loss: 144.0245\n",
            "====> Epoch: 47 Average loss: 166.1305 KL: 0.044570008473379996\n",
            "====> test loss: 142.9084\n",
            "====> Epoch: 48 Average loss: 165.2423 KL: 0.04363026954437849\n",
            "====> test loss: 145.2210\n",
            "====> Epoch: 49 Average loss: 165.0466 KL: 0.04457933509065267\n",
            "====> test loss: 142.3736\n",
            "====> Epoch: 50 Average loss: 164.7559 KL: 0.045718833248327165\n",
            "====> test loss: 142.5252\n",
            "====> Epoch: 51 Average loss: 132.0690 KL: 0.007764285411997421\n",
            "====> test loss: 135.5144\n",
            "====> Epoch: 52 Average loss: 133.8070 KL: 0.013430012094910465\n",
            "====> test loss: 135.7857\n",
            "====> Epoch: 53 Average loss: 135.4841 KL: 0.01889894447007148\n",
            "====> test loss: 135.2345\n",
            "====> Epoch: 54 Average loss: 138.0643 KL: 0.024161335075720686\n",
            "====> test loss: 136.4222\n",
            "====> Epoch: 55 Average loss: 140.5689 KL: 0.028685642958730013\n",
            "====> test loss: 137.4662\n",
            "====> Epoch: 56 Average loss: 143.5373 KL: 0.0332557300732926\n",
            "====> test loss: 137.3112\n",
            "====> Epoch: 57 Average loss: 146.2263 KL: 0.036817611517393774\n",
            "====> test loss: 137.4241\n",
            "====> Epoch: 58 Average loss: 149.0145 KL: 0.04099126485876397\n",
            "====> test loss: 138.7047\n",
            "====> Epoch: 59 Average loss: 151.8902 KL: 0.044619166068153526\n",
            "====> test loss: 136.8684\n",
            "====> Epoch: 60 Average loss: 154.2081 KL: 0.047705634721989534\n",
            "====> test loss: 140.7272\n",
            "====> Epoch: 61 Average loss: 156.9964 KL: 0.05117729921771275\n",
            "====> test loss: 139.9601\n",
            "====> Epoch: 62 Average loss: 159.4210 KL: 0.05335945851764933\n",
            "====> test loss: 139.0376\n",
            "====> Epoch: 63 Average loss: 159.4134 KL: 0.05457003960536022\n",
            "====> test loss: 139.1181\n",
            "====> Epoch: 64 Average loss: 158.6698 KL: 0.053860143870941596\n",
            "====> test loss: 139.1078\n",
            "====> Epoch: 65 Average loss: 159.1356 KL: 0.053945170214233874\n",
            "====> test loss: 140.8872\n",
            "====> Epoch: 66 Average loss: 158.4789 KL: 0.05521853066382622\n",
            "====> test loss: 138.0107\n",
            "====> Epoch: 67 Average loss: 158.2514 KL: 0.053790639943597236\n",
            "====> test loss: 140.1850\n",
            "====> Epoch: 68 Average loss: 158.5604 KL: 0.05408640468283153\n",
            "====> test loss: 139.7994\n",
            "====> Epoch: 69 Average loss: 157.9341 KL: 0.05421330266319639\n",
            "====> test loss: 139.2663\n",
            "====> Epoch: 70 Average loss: 157.9938 KL: 0.05592084739915806\n",
            "====> test loss: 138.8810\n",
            "====> Epoch: 71 Average loss: 157.4149 KL: 0.05645682841500355\n",
            "====> test loss: 140.1591\n",
            "====> Epoch: 72 Average loss: 157.4405 KL: 0.05479605496719433\n",
            "====> test loss: 137.6333\n",
            "====> Epoch: 73 Average loss: 157.0102 KL: 0.05540404024825562\n",
            "====> test loss: 140.1779\n",
            "====> Epoch: 74 Average loss: 156.8281 KL: 0.05752486522826278\n",
            "====> test loss: 137.6364\n",
            "====> Epoch: 75 Average loss: 156.7109 KL: 0.055571036096357246\n",
            "====> test loss: 139.1807\n",
            "====> Epoch: 76 Average loss: 120.7283 KL: 0.00914338674248407\n",
            "====> test loss: 132.6034\n",
            "====> Epoch: 77 Average loss: 122.7999 KL: 0.016287510673925303\n",
            "====> test loss: 131.2336\n",
            "====> Epoch: 78 Average loss: 124.9728 KL: 0.022545718912376694\n",
            "====> test loss: 132.6622\n",
            "====> Epoch: 79 Average loss: 127.9958 KL: 0.02916724979780978\n",
            "====> test loss: 132.5128\n",
            "====> Epoch: 80 Average loss: 131.2970 KL: 0.03476487604726497\n",
            "====> test loss: 132.3987\n",
            "====> Epoch: 81 Average loss: 134.0391 KL: 0.03957996519520554\n",
            "====> test loss: 132.4248\n",
            "====> Epoch: 82 Average loss: 137.3027 KL: 0.045150918644260546\n",
            "====> test loss: 134.7958\n",
            "====> Epoch: 83 Average loss: 140.5116 KL: 0.049327974086031325\n",
            "====> test loss: 135.5618\n",
            "====> Epoch: 84 Average loss: 143.3310 KL: 0.052701024561121856\n",
            "====> test loss: 136.3128\n",
            "====> Epoch: 85 Average loss: 146.2785 KL: 0.058512534569199214\n",
            "====> test loss: 135.9374\n",
            "====> Epoch: 86 Average loss: 149.3433 KL: 0.06128462332188617\n",
            "====> test loss: 136.3865\n",
            "====> Epoch: 87 Average loss: 152.2616 KL: 0.06706211556979863\n",
            "====> test loss: 136.9245\n",
            "====> Epoch: 88 Average loss: 152.2051 KL: 0.0640207046020348\n",
            "====> test loss: 140.4074\n",
            "====> Epoch: 89 Average loss: 152.2101 KL: 0.06445418094574959\n",
            "====> test loss: 135.1843\n",
            "====> Epoch: 90 Average loss: 151.0987 KL: 0.06599764821883751\n",
            "====> test loss: 136.4141\n",
            "====> Epoch: 91 Average loss: 151.7262 KL: 0.06674900522264095\n",
            "====> test loss: 137.8283\n",
            "====> Epoch: 92 Average loss: 151.0506 KL: 0.06632615767295538\n",
            "====> test loss: 138.0768\n",
            "====> Epoch: 93 Average loss: 150.8601 KL: 0.06667552887905318\n",
            "====> test loss: 138.4864\n",
            "====> Epoch: 94 Average loss: 151.0331 KL: 0.06773327330283621\n",
            "====> test loss: 133.2085\n",
            "====> Epoch: 95 Average loss: 150.7245 KL: 0.06704050256144893\n",
            "====> test loss: 135.4425\n",
            "====> Epoch: 96 Average loss: 150.4037 KL: 0.06775841798898406\n",
            "====> test loss: 134.4021\n",
            "====> Epoch: 97 Average loss: 149.9022 KL: 0.06798906194409729\n",
            "====> test loss: 137.2731\n",
            "====> Epoch: 98 Average loss: 150.0438 KL: 0.06876537830518156\n",
            "====> test loss: 136.0329\n",
            "====> Epoch: 99 Average loss: 149.4794 KL: 0.06821814051943421\n",
            "====> test loss: 135.2665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378VcPlbSF26"
      },
      "source": [
        "##eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXLXrsEQs1jj",
        "outputId": "b17a861d-9a28-4077-e302-94e4d7a6169f"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(train_index)), 300)\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "ids = []\n",
        "G_content = []\n",
        "scan_similarity = []\n",
        "input_similarity = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "l_di = dict(zip(ids, sets))\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "for iii, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  seq,_ = process_generated(x1)\n",
        "  seqsplit = split(seq)\n",
        "  Gcontent = 0\n",
        "  for i in range(len(seqsplit)):\n",
        "    a = seqsplit[i]\n",
        "    if a == 'T':\n",
        "      Gcontent +=1\n",
        "  G_content.append(Gcontent)\n",
        "  #print(y)\n",
        "  latent_space = vae.encode(x1)\n",
        "  ##print('\\n', latent_space)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "  yi = int(np.asarray(y.to('cpu')).argmax(1))\n",
        "  yid = l_di[yi]\n",
        "  if iii <10:\n",
        "    print('\\n >{} input \\n'.format(yid), seq) \n",
        "  \n",
        "  for ii in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = vae.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    if iii<10:\n",
        "      print('>{}{}_generated \\n'.format(yid, ii+1), gen_seq)\n",
        "    else:\n",
        "      pass\n",
        "    seqsplit = split(seq)\n",
        "    gensplit = split(gen_seq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    sim = tally/len(seqsplit)\n",
        "    mostsim = find_similar(gen_seq)\n",
        "    input_similarity.append(sim)\n",
        "    scan_similarity.append(mostsim)\n",
        "\n",
        "    if iii<10:\n",
        "      q = iii\n",
        "      print('% similarity', sim)\n",
        "      print('most similar in training set:',mostsim)\n",
        "    else:\n",
        "      pass\n",
        "      \n",
        "        \n",
        "\n",
        "      \n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "print('\\n \\n mean input similarity', mean(input_similarity))\n",
        "print('mean scanned similarity', mean(scan_similarity))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " >Glycine input \n",
            " GGCCCCGCGCAGGGAGAGACCGGCCAGGAGCCGGCGCCGAAGGAGCAACCGCCCCGGAAACTCTCAGGCACAAAGGACCGGCGCGGGACG\n",
            ">Glycine1_generated \n",
            " CCCCGCCCCGCGGGAGAGACTGGCTTCACGCCGGCGCCGAAGGAGCAACCGCCCCGGAAACTCTCAGGCAAACGGACCGGGCGGGGGCT\n",
            "% similarity 0.7444444444444445\n",
            "most similar in training set: 0.8089887640449438\n",
            "\n",
            " >Glycine input \n",
            " AAGATCAATACTGGAGAGCACGATGAGTCGTCGCCGAAGGAGCAAGCCGTCAAGGTGAATCTCTCAGGTAAAAGAACAGTATTGTGACG\n",
            ">Glycine1_generated \n",
            " ATGACTACAGCGGGAGAGACTACTTAAGTGGCGCCGAAGGAGCAAGCACTCCCCGTGAATCTCTCAGGCAAACGGACCGCTGTAGGACG\n",
            "% similarity 0.651685393258427\n",
            "most similar in training set: 0.8426966292134831\n",
            "\n",
            " >Glycine input \n",
            " TGATCTCGTGCGGGAGAGCACCCACACCAACCCAGTGTGGGTCACCGAAGGAGCAACTCCTCTCCGACAAACTCTCAGGTCCCCGTACCGCACGGGCCAG\n",
            ">Glycine1_generated \n",
            " CGATCCGCGGCGGGAGAGTCCCGCCCGCCCGCACCGGGCGGGCGCCGTAGGAGCAAATCCTCCCCGGGAATCTCTCAGGCACCCGGAACGCCGCCGACAGG\n",
            "% similarity 0.63\n",
            "most similar in training set: 0.8217821782178217\n",
            "\n",
            " >Glycine input \n",
            " CGGGGTCGAGCGGGAGAGTTCAGCGATTCGTCTGACGCCGAAGGAGCAATGTCCCGGAATCTCTCAGGCAAAGGGGACCGTTCGACACAA\n",
            ">Glycine1_generated \n",
            " TGTTCTCGTGCGGGAGAGATCGGCTTAAGCCGGCGCCGAAGGGGCAAACCCCCCCGGAAACTCTCAGGCAAAAGGAACGGCAGGGGAAAA\n",
            "% similarity 0.5444444444444444\n",
            "most similar in training set: 0.7777777777777778\n",
            "\n",
            " >Glycine input \n",
            " ATGACGTGTGCGGGAGAGCCCCGGCATCCAGCGTTTCTCGAGGATCCGGGCACCGAAGGAGCAAGCCTCCCCGCCAATCTCTCAGGTCCGCGTACCGCTCGCGTCCG\n",
            ">Glycine1_generated \n",
            " CAACCCGGTGCGGGAGAGCCCTGGCCCCGGGTACCAGGCGGGGTCGCCGGCGCCGAAGGGGCAAACCCCCCCGGAAACCTCTCAGGCACAAGACCCGCACCGGGTGG\n",
            "% similarity 0.6074766355140186\n",
            "most similar in training set: 0.6822429906542056\n",
            "\n",
            " >Glycine input \n",
            " GGACATAACCCTGGAGAGACCACTTAAATGTGGCATCAAAGGAGCAACTTTTTAAAGGAAACTTTCAGATAAAAGGACAGGGAGTAAAAT\n",
            ">Glycine1_generated \n",
            " GGATGAACCTCTGGAGAGACCCGTTAAGAGGGACGAAGGGGAAAGGTCTAAAGACCAAAATCTCTCAGGCAAAAGGACAGGAGAAACAAA\n",
            "% similarity 0.5555555555555556\n",
            "most similar in training set: 0.7333333333333333\n",
            "\n",
            " >Glycine input \n",
            " ATGAAGGCAGCGGGAGAGACTACCTGTAACACGGGGCGGCGCCGAAGGAGTAAGCCCTGGGGACAGGGTGAATCTCTCAGGCAAAAGGACCTTTGCCGGACG\n",
            ">Glycine1_generated \n",
            " ATGAAGGTAATGGGAGAGACCCAATCTTAAAACTGAAGGGGCAGGCGGTGAAGGAACAAGCTTTGGAGTGAATCTCTCAGGCAAAAGGACCTTTACTGGACG\n",
            "% similarity 0.6666666666666666\n",
            "most similar in training set: 0.7352941176470589\n",
            "\n",
            " >Glycine input \n",
            " ATGAAGAATTCGGGAGAGAACGCGAATGCGTCGCCGAAGGGGAACTCGCAAAAGCTCTCAGGCAAAAGTACCGGATTCCGACG\n",
            ">Glycine1_generated \n",
            " ATGAAGGATTCGGGAGAGACCGCATAGGCGGCGCCGAAGGGGAAAGAATCTAAGCTCACAGGAAAGAAGAGCGGAACGCGACCG\n",
            "% similarity 0.7228915662650602\n",
            "most similar in training set: 0.7142857142857143\n",
            "\n",
            " >Glycine input \n",
            " TACCTGTTTGCGGGAGAGAACAGCGAGAGCTGTCGCCGAAGGGGAAATCGCCCGAAATCTCTCAGGCAAAAGAACCGTAGACGGGAA\n",
            ">Glycine1_generated \n",
            " TACCTGTTTGCGGGAGAGAGCAGCGAGAGCTGCCGCCGAAGGGGAAATCGCCCGAAATCTCTCAGGCAAAAGAACCGTAGACGGGAA\n",
            "% similarity 0.9770114942528736\n",
            "most similar in training set: 1.0\n",
            "\n",
            " >Glycine input \n",
            " ATGAAGTTTGCGGGAGAGTGGCTAGAATAGCCCGCCGAAGGAGTAAAACTCTCAGGCACTCGGACCACCGAGTAGAA\n",
            ">Glycine1_generated \n",
            " GGAGCAAATTCTGGAGAGAGCGCATTTTCCGCCGCCGAAGGGTTCACAATCTCAGGCAAAAGGACAGAGGAGGAATAG\n",
            "% similarity 0.5454545454545454\n",
            "most similar in training set: 0.8846153846153846\n",
            "\n",
            " \n",
            " mean input similarity 0.6936282320650753\n",
            "mean scanned similarity 0.7835939238603502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b7SwGNOk5Mv"
      },
      "source": [
        ""
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "3Rt62PJH2xuy",
        "outputId": "7d0be7c4-9fd3-4dca-b471-32412c103f59"
      },
      "source": [
        "glycine1 =  torch.tensor([[ 0.8786, -2.5213,  1.3548,  0.4012, -0.8996, -1.0660, -0.1862,  1.4701,\n",
        "          0.0578, -1.3307,  1.1065,  1.7855, -0.1441,  0.4214,  1.1755,  0.0341]])\n",
        "glycine2 =  torch.tensor([[ 1.5450,  1.2961,  1.7923,  0.3352,  0.6877, -0.4763,  0.5722, -0.3662,\n",
        "         -1.3082, -0.0468, -2.6488, -1.4083, -0.1070, -0.9566, -0.1814,  1.7470]])\n",
        "dif = glycine2-glycine1\n",
        "\n",
        "steps = 5\n",
        "\n",
        "step = dif/steps\n",
        "yy = torch.tensor([[0,1,0]]).to(device).double()\n",
        "for i in range(steps+1):\n",
        "  latent_step = glycine1 + i*step\n",
        "  latent_step = latent_step.to(device).double()\n",
        "  gen = vae.generate_similar(latent_step, yy)\n",
        "  gen_seq, gen_code = process_generated(gen=gen)\n",
        "  print(gen_seq)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-4b80cb01195c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlatent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglycine1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mlatent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mgen_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-0ea8af4903c3>\u001b[0m in \u001b[0;36mgenerate_similar\u001b[0;34m(self, l, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m      \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m      \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m      \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdPvaXeEYPr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHYlSpUF3DiT"
      },
      "source": [
        "dif = glycine1-glycine2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "1kBsW8wKOwPR",
        "outputId": "f73484b5-f16c-436c-f8cb-5cbca6dda98f"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "G_total = G_content[0]\n",
        "for i in range(1,len(G_content)):\n",
        "  g = G_content[i]\n",
        " # G_total = np.concatenate((G_total, g), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=G_content, s=10)\n",
        "ax.set_title('Clustering by U Content')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 433.375x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkxxrQO8ODYy"
      },
      "source": [
        "# rnn vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwHxvIEyySCi"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7fqS7wxODYz"
      },
      "source": [
        " class RNN_VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, embedsize=8, num_classes=num_classes, hsize=256, num_layers = 1):\n",
        "        super(RNN_VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.embedsize = embedsize\n",
        "        self.num_classes = num_classes\n",
        "        self.drop_p = .3\n",
        "        self.num_layers = num_layers\n",
        "        self.hsize = hsize\n",
        "        \n",
        "        self.embed = nn.Embedding(4,self.embedsize)\n",
        "\n",
        "        self.gru = nn.GRU(input_size=self.embedsize, hidden_size= self.hsize, num_layers= num_layers, bidirectional= True, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.hsize*self.num_layers, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(128)\n",
        "        )\n",
        "\n",
        "        self.transform = nn.Sequential(nn.LeakyReLU())\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.hid1_dims+num_classes, 256), \n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        ind = x.argmax(2).type(torch.LongTensor)\n",
        "        ind = ind.to(device)\n",
        "        x = self.embed(ind)\n",
        "        x = x.double()\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2).contiguous()\n",
        "        # Encode input\n",
        "        #h = torch.cat([h,y], dim=1)\n",
        "        h2 = self.encoder(h)\n",
        "        #h = self.transform(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2) \n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(h) ####h\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(h) ####h\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(h) ####z\n",
        "      return z \n",
        "\n",
        "\n",
        "    def encode(self, x,y):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        ind = x.argmax(2).type(torch.LongTensor)\n",
        "        ind = ind.to(device)\n",
        "        x = self.embed(ind)\n",
        "        x = x.double()\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "       # h = torch.cat([h,y], dim=1)\n",
        "        h2 = self.encoder(h)\n",
        "        h = self.transform(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        l2 = self._reparameterize(mu, logvar)\n",
        "        return l2\n",
        "\n",
        " #   def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "  #      gru_out, h = self.gru(x) ##, self.hidden)\n",
        "   #     h = h.permute(1,2,0)\n",
        "    #    h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "     #   h2 = self.encoder(h)\n",
        "      #  mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "       # hx = self._reparameterize(mu, logvar)\n",
        "        #return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwDLmn22w60S"
      },
      "source": [
        "##RNNVAE loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kTWCcUnODY2"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def rnnvae_loss_function(recon_x, x, mu, log_var, epo):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())*cyclic_anneal(epoch, vae_epochs, cycles)\n",
        "    return BCE + 1.5*KLD #see Serena Yeung et. al. for KL scaling factor explanation\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MaXNX3Aw-1Z"
      },
      "source": [
        "##train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqK7hbb-ODY8"
      },
      "source": [
        "def train_rnn_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  rnnvae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(train_loader):\n",
        "      x = x.double().to(device)\n",
        "      l = l.to('cpu')\n",
        "      pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = rnnvae(x,c)\n",
        "      loss = rnnvae_loss_function(recon_batch, x, mu, log_var, epoch)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test_rnnvae(epoch, loader):\n",
        "  test_load = loader\n",
        "  rnnvae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      l = l.to('cpu')\n",
        "      pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "      recon_batch, mu, log_var = rnnvae(x,c)\n",
        "     ## loss = F.mse_loss(x.flatten(), recon_batch.flatten(), reduction = 'sum')\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sov2kEXxGCE"
      },
      "source": [
        "##hyperparam, build, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwyap361ODY-"
      },
      "source": [
        "batch_size_vae =  200\n",
        "vae_epochs = 300\n",
        "latent_size = 16\n",
        "cycles = 10"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89eDJA7ODY_"
      },
      "source": [
        "# build model\n",
        "\n",
        "rnnvae = RNN_VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, embedsize= 4, num_classes=num_classes, hsize=150, num_layers=1).double()\n",
        "rnnvae = rnnvae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(rnnvae.parameters(), lr = .001)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgP82nmBODZB"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0xCWPAkODZD",
        "outputId": "564e6dcd-8ddb-40e6-e763-913869b6c3e0"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_rnn_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_rnnvae(epoch, vae_test_loader)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 0 Average loss: 205.7485\n",
            "====> test loss: 197.8079\n",
            "====> Epoch: 1 Average loss: 178.7751\n",
            "====> test loss: 175.4062\n",
            "====> Epoch: 2 Average loss: 174.9705\n",
            "====> test loss: 172.3601\n",
            "====> Epoch: 3 Average loss: 173.3021\n",
            "====> test loss: 169.2831\n",
            "====> Epoch: 4 Average loss: 171.7949\n",
            "====> test loss: 168.7139\n",
            "====> Epoch: 5 Average loss: 170.6125\n",
            "====> test loss: 168.3831\n",
            "====> Epoch: 6 Average loss: 169.3544\n",
            "====> test loss: 165.4524\n",
            "====> Epoch: 7 Average loss: 168.6170\n",
            "====> test loss: 164.6911\n",
            "====> Epoch: 8 Average loss: 167.7456\n",
            "====> test loss: 164.6482\n",
            "====> Epoch: 9 Average loss: 167.2215\n",
            "====> test loss: 164.9461\n",
            "====> Epoch: 10 Average loss: 166.5682\n",
            "====> test loss: 161.9267\n",
            "====> Epoch: 11 Average loss: 166.4362\n",
            "====> test loss: 160.7019\n",
            "====> Epoch: 12 Average loss: 166.0647\n",
            "====> test loss: 160.8958\n",
            "====> Epoch: 13 Average loss: 165.6598\n",
            "====> test loss: 159.1403\n",
            "====> Epoch: 14 Average loss: 165.4664\n",
            "====> test loss: 160.9619\n",
            "====> Epoch: 15 Average loss: 165.1056\n",
            "====> test loss: 159.2967\n",
            "====> Epoch: 16 Average loss: 164.2896\n",
            "====> test loss: 160.0741\n",
            "====> Epoch: 17 Average loss: 163.7295\n",
            "====> test loss: 163.6403\n",
            "====> Epoch: 18 Average loss: 163.4280\n",
            "====> test loss: 160.4798\n",
            "====> Epoch: 19 Average loss: 162.6389\n",
            "====> test loss: 158.5082\n",
            "====> Epoch: 20 Average loss: 162.4452\n",
            "====> test loss: 158.7995\n",
            "====> Epoch: 21 Average loss: 162.0237\n",
            "====> test loss: 158.4359\n",
            "====> Epoch: 22 Average loss: 161.7940\n",
            "====> test loss: 157.0603\n",
            "====> Epoch: 23 Average loss: 161.0257\n",
            "====> test loss: 158.0760\n",
            "====> Epoch: 24 Average loss: 160.3535\n",
            "====> test loss: 155.2095\n",
            "====> Epoch: 25 Average loss: 159.8009\n",
            "====> test loss: 156.1112\n",
            "====> Epoch: 26 Average loss: 159.3637\n",
            "====> test loss: 155.4008\n",
            "====> Epoch: 27 Average loss: 158.9276\n",
            "====> test loss: 153.3812\n",
            "====> Epoch: 28 Average loss: 158.8316\n",
            "====> test loss: 153.4769\n",
            "====> Epoch: 29 Average loss: 158.2637\n",
            "====> test loss: 155.5157\n",
            "====> Epoch: 30 Average loss: 158.1198\n",
            "====> test loss: 154.2231\n",
            "====> Epoch: 31 Average loss: 147.9018\n",
            "====> test loss: 151.1923\n",
            "====> Epoch: 32 Average loss: 147.3106\n",
            "====> test loss: 150.3626\n",
            "====> Epoch: 33 Average loss: 147.3558\n",
            "====> test loss: 151.7516\n",
            "====> Epoch: 34 Average loss: 147.5443\n",
            "====> test loss: 150.2348\n",
            "====> Epoch: 35 Average loss: 147.8074\n",
            "====> test loss: 150.1518\n",
            "====> Epoch: 36 Average loss: 148.2397\n",
            "====> test loss: 152.9846\n",
            "====> Epoch: 37 Average loss: 148.5057\n",
            "====> test loss: 153.5701\n",
            "====> Epoch: 38 Average loss: 148.7650\n",
            "====> test loss: 151.3487\n",
            "====> Epoch: 39 Average loss: 149.3298\n",
            "====> test loss: 151.0041\n",
            "====> Epoch: 40 Average loss: 149.4102\n",
            "====> test loss: 148.5957\n",
            "====> Epoch: 41 Average loss: 149.4881\n",
            "====> test loss: 149.2597\n",
            "====> Epoch: 42 Average loss: 149.7613\n",
            "====> test loss: 148.5802\n",
            "====> Epoch: 43 Average loss: 150.0351\n",
            "====> test loss: 147.9243\n",
            "====> Epoch: 44 Average loss: 150.2456\n",
            "====> test loss: 148.8598\n",
            "====> Epoch: 45 Average loss: 150.7439\n",
            "====> test loss: 147.1302\n",
            "====> Epoch: 46 Average loss: 150.3312\n",
            "====> test loss: 147.0231\n",
            "====> Epoch: 47 Average loss: 149.9167\n",
            "====> test loss: 146.0825\n",
            "====> Epoch: 48 Average loss: 149.4700\n",
            "====> test loss: 146.9026\n",
            "====> Epoch: 49 Average loss: 148.8581\n",
            "====> test loss: 148.3654\n",
            "====> Epoch: 50 Average loss: 148.7931\n",
            "====> test loss: 148.4338\n",
            "====> Epoch: 51 Average loss: 148.4978\n",
            "====> test loss: 147.1741\n",
            "====> Epoch: 52 Average loss: 149.0204\n",
            "====> test loss: 150.2107\n",
            "====> Epoch: 53 Average loss: 148.4636\n",
            "====> test loss: 147.1363\n",
            "====> Epoch: 54 Average loss: 147.2777\n",
            "====> test loss: 145.1115\n",
            "====> Epoch: 55 Average loss: 146.5666\n",
            "====> test loss: 145.9714\n",
            "====> Epoch: 56 Average loss: 146.1302\n",
            "====> test loss: 144.5200\n",
            "====> Epoch: 57 Average loss: 146.0253\n",
            "====> test loss: 146.3766\n",
            "====> Epoch: 58 Average loss: 145.4839\n",
            "====> test loss: 144.6583\n",
            "====> Epoch: 59 Average loss: 145.2659\n",
            "====> test loss: 143.8260\n",
            "====> Epoch: 60 Average loss: 144.5163\n",
            "====> test loss: 143.9506\n",
            "====> Epoch: 61 Average loss: 132.5189\n",
            "====> test loss: 143.4975\n",
            "====> Epoch: 62 Average loss: 131.9938\n",
            "====> test loss: 141.5646\n",
            "====> Epoch: 63 Average loss: 132.4574\n",
            "====> test loss: 141.6082\n",
            "====> Epoch: 64 Average loss: 133.0008\n",
            "====> test loss: 142.8087\n",
            "====> Epoch: 65 Average loss: 133.7118\n",
            "====> test loss: 144.0518\n",
            "====> Epoch: 66 Average loss: 134.4719\n",
            "====> test loss: 142.4964\n",
            "====> Epoch: 67 Average loss: 135.1976\n",
            "====> test loss: 141.9034\n",
            "====> Epoch: 68 Average loss: 136.3447\n",
            "====> test loss: 144.4884\n",
            "====> Epoch: 69 Average loss: 137.5878\n",
            "====> test loss: 146.1889\n",
            "====> Epoch: 70 Average loss: 138.8401\n",
            "====> test loss: 146.8650\n",
            "====> Epoch: 71 Average loss: 138.6335\n",
            "====> test loss: 145.0222\n",
            "====> Epoch: 72 Average loss: 139.1561\n",
            "====> test loss: 144.3562\n",
            "====> Epoch: 73 Average loss: 139.9174\n",
            "====> test loss: 146.3403\n",
            "====> Epoch: 74 Average loss: 142.4171\n",
            "====> test loss: 153.5126\n",
            "====> Epoch: 75 Average loss: 141.5853\n",
            "====> test loss: 144.0802\n",
            "====> Epoch: 76 Average loss: 140.3404\n",
            "====> test loss: 143.1209\n",
            "====> Epoch: 77 Average loss: 140.3472\n",
            "====> test loss: 143.9146\n",
            "====> Epoch: 78 Average loss: 140.6314\n",
            "====> test loss: 145.3597\n",
            "====> Epoch: 79 Average loss: 141.1654\n",
            "====> test loss: 144.4313\n",
            "====> Epoch: 80 Average loss: 139.3642\n",
            "====> test loss: 145.2699\n",
            "====> Epoch: 81 Average loss: 138.3000\n",
            "====> test loss: 142.5990\n",
            "====> Epoch: 82 Average loss: 138.0857\n",
            "====> test loss: 142.1824\n",
            "====> Epoch: 83 Average loss: 137.5718\n",
            "====> test loss: 144.4275\n",
            "====> Epoch: 84 Average loss: 137.2157\n",
            "====> test loss: 147.3769\n",
            "====> Epoch: 85 Average loss: 137.1297\n",
            "====> test loss: 144.8293\n",
            "====> Epoch: 86 Average loss: 137.2685\n",
            "====> test loss: 148.1769\n",
            "====> Epoch: 87 Average loss: 137.5903\n",
            "====> test loss: 148.9696\n",
            "====> Epoch: 88 Average loss: 138.2111\n",
            "====> test loss: 156.9784\n",
            "====> Epoch: 89 Average loss: 141.4296\n",
            "====> test loss: 156.6132\n",
            "====> Epoch: 90 Average loss: 142.2942\n",
            "====> test loss: 145.8150\n",
            "====> Epoch: 91 Average loss: 124.9924\n",
            "====> test loss: 139.6611\n",
            "====> Epoch: 92 Average loss: 123.3822\n",
            "====> test loss: 139.2357\n",
            "====> Epoch: 93 Average loss: 123.7850\n",
            "====> test loss: 139.1828\n",
            "====> Epoch: 94 Average loss: 124.3741\n",
            "====> test loss: 140.3553\n",
            "====> Epoch: 95 Average loss: 124.9113\n",
            "====> test loss: 141.2714\n",
            "====> Epoch: 96 Average loss: 125.5499\n",
            "====> test loss: 142.4778\n",
            "====> Epoch: 97 Average loss: 125.9848\n",
            "====> test loss: 141.5507\n",
            "====> Epoch: 98 Average loss: 126.6715\n",
            "====> test loss: 141.8014\n",
            "====> Epoch: 99 Average loss: 127.2750\n",
            "====> test loss: 140.6960\n",
            "====> Epoch: 100 Average loss: 127.9528\n",
            "====> test loss: 141.5095\n",
            "====> Epoch: 101 Average loss: 128.6745\n",
            "====> test loss: 142.0939\n",
            "====> Epoch: 102 Average loss: 129.4540\n",
            "====> test loss: 142.3753\n",
            "====> Epoch: 103 Average loss: 130.4402\n",
            "====> test loss: 144.2957\n",
            "====> Epoch: 104 Average loss: 131.5138\n",
            "====> test loss: 145.6915\n",
            "====> Epoch: 105 Average loss: 132.4224\n",
            "====> test loss: 148.5744\n",
            "====> Epoch: 106 Average loss: 132.9616\n",
            "====> test loss: 149.1247\n",
            "====> Epoch: 107 Average loss: 132.9227\n",
            "====> test loss: 149.5114\n",
            "====> Epoch: 108 Average loss: 132.5255\n",
            "====> test loss: 149.5200\n",
            "====> Epoch: 109 Average loss: 132.5043\n",
            "====> test loss: 146.7699\n",
            "====> Epoch: 110 Average loss: 131.6143\n",
            "====> test loss: 147.0734\n",
            "====> Epoch: 111 Average loss: 132.2138\n",
            "====> test loss: 147.8988\n",
            "====> Epoch: 112 Average loss: 132.0336\n",
            "====> test loss: 146.7231\n",
            "====> Epoch: 113 Average loss: 132.7536\n",
            "====> test loss: 145.9817\n",
            "====> Epoch: 114 Average loss: 133.2887\n",
            "====> test loss: 151.9141\n",
            "====> Epoch: 115 Average loss: 133.7457\n",
            "====> test loss: 151.8183\n",
            "====> Epoch: 116 Average loss: 133.1460\n",
            "====> test loss: 144.3525\n",
            "====> Epoch: 117 Average loss: 131.9035\n",
            "====> test loss: 143.6848\n",
            "====> Epoch: 118 Average loss: 130.7827\n",
            "====> test loss: 143.7014\n",
            "====> Epoch: 119 Average loss: 130.2146\n",
            "====> test loss: 142.9545\n",
            "====> Epoch: 120 Average loss: 130.6012\n",
            "====> test loss: 141.0598\n",
            "====> Epoch: 121 Average loss: 117.3340\n",
            "====> test loss: 142.3487\n",
            "====> Epoch: 122 Average loss: 117.7210\n",
            "====> test loss: 142.3173\n",
            "====> Epoch: 123 Average loss: 118.6362\n",
            "====> test loss: 146.7965\n",
            "====> Epoch: 124 Average loss: 119.5650\n",
            "====> test loss: 148.6328\n",
            "====> Epoch: 125 Average loss: 120.3811\n",
            "====> test loss: 146.9465\n",
            "====> Epoch: 126 Average loss: 121.7734\n",
            "====> test loss: 145.9338\n",
            "====> Epoch: 127 Average loss: 122.6668\n",
            "====> test loss: 146.4144\n",
            "====> Epoch: 128 Average loss: 124.1315\n",
            "====> test loss: 143.9455\n",
            "====> Epoch: 129 Average loss: 125.0459\n",
            "====> test loss: 144.4762\n",
            "====> Epoch: 130 Average loss: 125.3209\n",
            "====> test loss: 144.8550\n",
            "====> Epoch: 131 Average loss: 125.7619\n",
            "====> test loss: 142.3213\n",
            "====> Epoch: 132 Average loss: 125.8484\n",
            "====> test loss: 140.8132\n",
            "====> Epoch: 133 Average loss: 126.1446\n",
            "====> test loss: 142.5089\n",
            "====> Epoch: 134 Average loss: 126.4529\n",
            "====> test loss: 140.6985\n",
            "====> Epoch: 135 Average loss: 126.8769\n",
            "====> test loss: 141.9186\n",
            "====> Epoch: 136 Average loss: 126.0867\n",
            "====> test loss: 137.9008\n",
            "====> Epoch: 137 Average loss: 125.7860\n",
            "====> test loss: 139.1355\n",
            "====> Epoch: 138 Average loss: 125.2373\n",
            "====> test loss: 141.7441\n",
            "====> Epoch: 139 Average loss: 125.0131\n",
            "====> test loss: 141.4790\n",
            "====> Epoch: 140 Average loss: 124.7466\n",
            "====> test loss: 138.4424\n",
            "====> Epoch: 141 Average loss: 124.5822\n",
            "====> test loss: 141.5895\n",
            "====> Epoch: 142 Average loss: 124.5155\n",
            "====> test loss: 139.7829\n",
            "====> Epoch: 143 Average loss: 124.0589\n",
            "====> test loss: 143.3447\n",
            "====> Epoch: 144 Average loss: 124.0196\n",
            "====> test loss: 142.5937\n",
            "====> Epoch: 145 Average loss: 124.1797\n",
            "====> test loss: 141.9093\n",
            "====> Epoch: 146 Average loss: 124.0435\n",
            "====> test loss: 141.6228\n",
            "====> Epoch: 147 Average loss: 124.2210\n",
            "====> test loss: 144.1817\n",
            "====> Epoch: 148 Average loss: 124.6897\n",
            "====> test loss: 144.1793\n",
            "====> Epoch: 149 Average loss: 125.2839\n",
            "====> test loss: 145.8472\n",
            "====> Epoch: 150 Average loss: 126.0381\n",
            "====> test loss: 153.3800\n",
            "====> Epoch: 151 Average loss: 112.8357\n",
            "====> test loss: 168.0887\n",
            "====> Epoch: 152 Average loss: 113.8553\n",
            "====> test loss: 146.2213\n",
            "====> Epoch: 153 Average loss: 113.6087\n",
            "====> test loss: 140.4487\n",
            "====> Epoch: 154 Average loss: 112.6061\n",
            "====> test loss: 146.7354\n",
            "====> Epoch: 155 Average loss: 112.6696\n",
            "====> test loss: 150.4530\n",
            "====> Epoch: 156 Average loss: 113.7502\n",
            "====> test loss: 146.8768\n",
            "====> Epoch: 157 Average loss: 114.5623\n",
            "====> test loss: 145.7731\n",
            "====> Epoch: 158 Average loss: 115.6642\n",
            "====> test loss: 150.0910\n",
            "====> Epoch: 159 Average loss: 116.5779\n",
            "====> test loss: 152.0082\n",
            "====> Epoch: 160 Average loss: 117.7339\n",
            "====> test loss: 147.6124\n",
            "====> Epoch: 161 Average loss: 118.8000\n",
            "====> test loss: 147.2669\n",
            "====> Epoch: 162 Average loss: 120.1326\n",
            "====> test loss: 148.6785\n",
            "====> Epoch: 163 Average loss: 121.5000\n",
            "====> test loss: 150.5674\n",
            "====> Epoch: 164 Average loss: 122.2885\n",
            "====> test loss: 151.0888\n",
            "====> Epoch: 165 Average loss: 123.3659\n",
            "====> test loss: 155.4944\n",
            "====> Epoch: 166 Average loss: 124.1076\n",
            "====> test loss: 150.4450\n",
            "====> Epoch: 167 Average loss: 124.9539\n",
            "====> test loss: 151.9469\n",
            "====> Epoch: 168 Average loss: 126.5717\n",
            "====> test loss: 153.5046\n",
            "====> Epoch: 169 Average loss: 129.4399\n",
            "====> test loss: 149.0457\n",
            "====> Epoch: 170 Average loss: 129.1423\n",
            "====> test loss: 152.5949\n",
            "====> Epoch: 171 Average loss: 126.6013\n",
            "====> test loss: 147.6264\n",
            "====> Epoch: 172 Average loss: 124.0141\n",
            "====> test loss: 143.3231\n",
            "====> Epoch: 173 Average loss: 122.2133\n",
            "====> test loss: 144.2888\n",
            "====> Epoch: 174 Average loss: 122.1537\n",
            "====> test loss: 145.6268\n",
            "====> Epoch: 175 Average loss: 122.6941\n",
            "====> test loss: 147.1399\n",
            "====> Epoch: 176 Average loss: 123.6360\n",
            "====> test loss: 157.2349\n",
            "====> Epoch: 177 Average loss: 125.1808\n",
            "====> test loss: 163.7216\n",
            "====> Epoch: 178 Average loss: 126.4423\n",
            "====> test loss: 156.5648\n",
            "====> Epoch: 179 Average loss: 124.8155\n",
            "====> test loss: 148.0148\n",
            "====> Epoch: 180 Average loss: 121.1560\n",
            "====> test loss: 144.6902\n",
            "====> Epoch: 181 Average loss: 104.2068\n",
            "====> test loss: 142.0627\n",
            "====> Epoch: 182 Average loss: 103.7674\n",
            "====> test loss: 142.1636\n",
            "====> Epoch: 183 Average loss: 104.4389\n",
            "====> test loss: 142.2404\n",
            "====> Epoch: 184 Average loss: 105.3646\n",
            "====> test loss: 142.6891\n",
            "====> Epoch: 185 Average loss: 106.2995\n",
            "====> test loss: 144.2016\n",
            "====> Epoch: 186 Average loss: 107.2373\n",
            "====> test loss: 144.1020\n",
            "====> Epoch: 187 Average loss: 108.2211\n",
            "====> test loss: 144.8687\n",
            "====> Epoch: 188 Average loss: 109.1695\n",
            "====> test loss: 145.6079\n",
            "====> Epoch: 189 Average loss: 110.0823\n",
            "====> test loss: 145.6852\n",
            "====> Epoch: 190 Average loss: 111.0339\n",
            "====> test loss: 146.2965\n",
            "====> Epoch: 191 Average loss: 111.7997\n",
            "====> test loss: 149.3512\n",
            "====> Epoch: 192 Average loss: 112.9996\n",
            "====> test loss: 149.5366\n",
            "====> Epoch: 193 Average loss: 113.9375\n",
            "====> test loss: 149.9257\n",
            "====> Epoch: 194 Average loss: 115.1040\n",
            "====> test loss: 149.7042\n",
            "====> Epoch: 195 Average loss: 115.8137\n",
            "====> test loss: 151.4332\n",
            "====> Epoch: 196 Average loss: 115.9451\n",
            "====> test loss: 149.9597\n",
            "====> Epoch: 197 Average loss: 115.6877\n",
            "====> test loss: 147.5298\n",
            "====> Epoch: 198 Average loss: 115.5341\n",
            "====> test loss: 148.8142\n",
            "====> Epoch: 199 Average loss: 115.6741\n",
            "====> test loss: 149.8418\n",
            "====> Epoch: 200 Average loss: 115.9780\n",
            "====> test loss: 151.4333\n",
            "====> Epoch: 201 Average loss: 115.9223\n",
            "====> test loss: 149.9000\n",
            "====> Epoch: 202 Average loss: 116.2198\n",
            "====> test loss: 149.6586\n",
            "====> Epoch: 203 Average loss: 116.7154\n",
            "====> test loss: 148.1476\n",
            "====> Epoch: 204 Average loss: 116.8077\n",
            "====> test loss: 150.5388\n",
            "====> Epoch: 205 Average loss: 117.3148\n",
            "====> test loss: 151.2191\n",
            "====> Epoch: 206 Average loss: 118.0461\n",
            "====> test loss: 151.4031\n",
            "====> Epoch: 207 Average loss: 117.7184\n",
            "====> test loss: 147.4494\n",
            "====> Epoch: 208 Average loss: 116.4930\n",
            "====> test loss: 146.1469\n",
            "====> Epoch: 209 Average loss: 114.9421\n",
            "====> test loss: 147.6221\n",
            "====> Epoch: 210 Average loss: 114.1576\n",
            "====> test loss: 143.4788\n",
            "====> Epoch: 211 Average loss: 98.3424\n",
            "====> test loss: 144.9993\n",
            "====> Epoch: 212 Average loss: 98.6779\n",
            "====> test loss: 146.4103\n",
            "====> Epoch: 213 Average loss: 99.6021\n",
            "====> test loss: 145.7043\n",
            "====> Epoch: 214 Average loss: 100.8933\n",
            "====> test loss: 146.1749\n",
            "====> Epoch: 215 Average loss: 102.2680\n",
            "====> test loss: 146.1461\n",
            "====> Epoch: 216 Average loss: 103.4634\n",
            "====> test loss: 146.1630\n",
            "====> Epoch: 217 Average loss: 104.9185\n",
            "====> test loss: 146.2653\n",
            "====> Epoch: 218 Average loss: 106.2416\n",
            "====> test loss: 147.0959\n",
            "====> Epoch: 219 Average loss: 107.8322\n",
            "====> test loss: 147.6872\n",
            "====> Epoch: 220 Average loss: 109.3027\n",
            "====> test loss: 149.4356\n",
            "====> Epoch: 221 Average loss: 111.3527\n",
            "====> test loss: 148.5222\n",
            "====> Epoch: 222 Average loss: 112.8435\n",
            "====> test loss: 151.4594\n",
            "====> Epoch: 223 Average loss: 114.4628\n",
            "====> test loss: 154.2529\n",
            "====> Epoch: 224 Average loss: 115.5748\n",
            "====> test loss: 152.3083\n",
            "====> Epoch: 225 Average loss: 116.7974\n",
            "====> test loss: 147.0488\n",
            "====> Epoch: 226 Average loss: 116.2614\n",
            "====> test loss: 150.1488\n",
            "====> Epoch: 227 Average loss: 116.3096\n",
            "====> test loss: 150.5818\n",
            "====> Epoch: 228 Average loss: 117.3877\n",
            "====> test loss: 150.2074\n",
            "====> Epoch: 229 Average loss: 117.5085\n",
            "====> test loss: 151.0895\n",
            "====> Epoch: 230 Average loss: 118.1982\n",
            "====> test loss: 152.5509\n",
            "====> Epoch: 231 Average loss: 118.6749\n",
            "====> test loss: 163.3104\n",
            "====> Epoch: 232 Average loss: 118.5029\n",
            "====> test loss: 165.7690\n",
            "====> Epoch: 233 Average loss: 118.5799\n",
            "====> test loss: 164.3274\n",
            "====> Epoch: 234 Average loss: 117.3804\n",
            "====> test loss: 151.5398\n",
            "====> Epoch: 235 Average loss: 114.8697\n",
            "====> test loss: 147.3442\n",
            "====> Epoch: 236 Average loss: 112.5255\n",
            "====> test loss: 146.1370\n",
            "====> Epoch: 237 Average loss: 111.2244\n",
            "====> test loss: 146.6010\n",
            "====> Epoch: 238 Average loss: 110.6584\n",
            "====> test loss: 147.8870\n",
            "====> Epoch: 239 Average loss: 110.3577\n",
            "====> test loss: 152.4714\n",
            "====> Epoch: 240 Average loss: 110.2170\n",
            "====> test loss: 151.3037\n",
            "====> Epoch: 241 Average loss: 94.3161\n",
            "====> test loss: 147.4904\n",
            "====> Epoch: 242 Average loss: 94.3906\n",
            "====> test loss: 145.6066\n",
            "====> Epoch: 243 Average loss: 95.1536\n",
            "====> test loss: 146.9727\n",
            "====> Epoch: 244 Average loss: 96.3362\n",
            "====> test loss: 147.1395\n",
            "====> Epoch: 245 Average loss: 97.5406\n",
            "====> test loss: 147.0365\n",
            "====> Epoch: 246 Average loss: 98.9404\n",
            "====> test loss: 147.6993\n",
            "====> Epoch: 247 Average loss: 100.1261\n",
            "====> test loss: 147.2268\n",
            "====> Epoch: 248 Average loss: 101.3281\n",
            "====> test loss: 148.8748\n",
            "====> Epoch: 249 Average loss: 102.6053\n",
            "====> test loss: 149.1117\n",
            "====> Epoch: 250 Average loss: 104.1719\n",
            "====> test loss: 147.9517\n",
            "====> Epoch: 251 Average loss: 105.9784\n",
            "====> test loss: 148.0784\n",
            "====> Epoch: 252 Average loss: 107.2352\n",
            "====> test loss: 149.4568\n",
            "====> Epoch: 253 Average loss: 108.9163\n",
            "====> test loss: 154.0195\n",
            "====> Epoch: 254 Average loss: 110.3604\n",
            "====> test loss: 157.3634\n",
            "====> Epoch: 255 Average loss: 110.7579\n",
            "====> test loss: 150.5276\n",
            "====> Epoch: 256 Average loss: 109.5339\n",
            "====> test loss: 150.9421\n",
            "====> Epoch: 257 Average loss: 108.7152\n",
            "====> test loss: 153.9834\n",
            "====> Epoch: 258 Average loss: 108.1695\n",
            "====> test loss: 149.2284\n",
            "====> Epoch: 259 Average loss: 107.8775\n",
            "====> test loss: 153.5742\n",
            "====> Epoch: 260 Average loss: 107.7270\n",
            "====> test loss: 150.7233\n",
            "====> Epoch: 261 Average loss: 107.6317\n",
            "====> test loss: 152.0700\n",
            "====> Epoch: 262 Average loss: 107.7001\n",
            "====> test loss: 154.0790\n",
            "====> Epoch: 263 Average loss: 107.5198\n",
            "====> test loss: 152.7177\n",
            "====> Epoch: 264 Average loss: 107.6139\n",
            "====> test loss: 152.4198\n",
            "====> Epoch: 265 Average loss: 107.4387\n",
            "====> test loss: 154.9276\n",
            "====> Epoch: 266 Average loss: 107.5126\n",
            "====> test loss: 154.7947\n",
            "====> Epoch: 267 Average loss: 107.4368\n",
            "====> test loss: 157.2694\n",
            "====> Epoch: 268 Average loss: 107.2348\n",
            "====> test loss: 155.0151\n",
            "====> Epoch: 269 Average loss: 107.0858\n",
            "====> test loss: 157.2056\n",
            "====> Epoch: 270 Average loss: 106.7991\n",
            "====> test loss: 156.0600\n",
            "====> Epoch: 271 Average loss: 90.4578\n",
            "====> test loss: 152.5057\n",
            "====> Epoch: 272 Average loss: 90.4823\n",
            "====> test loss: 153.3201\n",
            "====> Epoch: 273 Average loss: 91.4925\n",
            "====> test loss: 153.4899\n",
            "====> Epoch: 274 Average loss: 92.6803\n",
            "====> test loss: 154.0610\n",
            "====> Epoch: 275 Average loss: 93.8442\n",
            "====> test loss: 156.6559\n",
            "====> Epoch: 276 Average loss: 95.0941\n",
            "====> test loss: 156.3197\n",
            "====> Epoch: 277 Average loss: 96.4015\n",
            "====> test loss: 155.7823\n",
            "====> Epoch: 278 Average loss: 97.6632\n",
            "====> test loss: 159.1332\n",
            "====> Epoch: 279 Average loss: 98.9854\n",
            "====> test loss: 159.2164\n",
            "====> Epoch: 280 Average loss: 100.3773\n",
            "====> test loss: 157.3221\n",
            "====> Epoch: 281 Average loss: 102.0477\n",
            "====> test loss: 158.4552\n",
            "====> Epoch: 282 Average loss: 103.2442\n",
            "====> test loss: 166.8618\n",
            "====> Epoch: 283 Average loss: 104.6294\n",
            "====> test loss: 168.7555\n",
            "====> Epoch: 284 Average loss: 106.4379\n",
            "====> test loss: 164.9861\n",
            "====> Epoch: 285 Average loss: 107.7034\n",
            "====> test loss: 171.2020\n",
            "====> Epoch: 286 Average loss: 108.0214\n",
            "====> test loss: 174.2490\n",
            "====> Epoch: 287 Average loss: 108.8760\n",
            "====> test loss: 164.0541\n",
            "====> Epoch: 288 Average loss: 109.7163\n",
            "====> test loss: 158.1152\n",
            "====> Epoch: 289 Average loss: 110.2481\n",
            "====> test loss: 163.8482\n",
            "====> Epoch: 290 Average loss: 111.0669\n",
            "====> test loss: 161.3835\n",
            "====> Epoch: 291 Average loss: 111.4383\n",
            "====> test loss: 163.9843\n",
            "====> Epoch: 292 Average loss: 112.8081\n",
            "====> test loss: 163.5602\n",
            "====> Epoch: 293 Average loss: 113.1941\n",
            "====> test loss: 164.3046\n",
            "====> Epoch: 294 Average loss: 112.6564\n",
            "====> test loss: 160.3343\n",
            "====> Epoch: 295 Average loss: 112.3727\n",
            "====> test loss: 154.2408\n",
            "====> Epoch: 296 Average loss: 109.8952\n",
            "====> test loss: 159.1797\n",
            "====> Epoch: 297 Average loss: 106.7793\n",
            "====> test loss: 155.8755\n",
            "====> Epoch: 298 Average loss: 104.8906\n",
            "====> test loss: 155.7295\n",
            "====> Epoch: 299 Average loss: 103.8738\n",
            "====> test loss: 154.4123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFrmZLMxMB0"
      },
      "source": [
        "##EVAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVoWWgY9toLd",
        "outputId": "25282098-9a39-4abf-9b4f-37cac82b75ab"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(train_index)), 10)\n",
        "\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "ids = []\n",
        "scan_similarity = []\n",
        "input_similarity = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "\n",
        "for iii, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  seq,_ = process_generated(x1)\n",
        "  #print(y)\n",
        "  latent_space = rnnvae.encode(x,y)\n",
        "  print('\\n', latent_space)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "  yi = int(np.asarray(y.to('cpu')).argmax(1))\n",
        "  yid = l_di[yi]\n",
        "  if iii <10:\n",
        "    print('\\n >{} input \\n'.format(yid), seq) \n",
        "  \n",
        "  for ii in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    if iii<10:\n",
        "      print('>{} generated: {} \\n'.format(yid, ii+1), gen_seq)\n",
        "    else:\n",
        "      pass\n",
        "    seqsplit = split(seq)\n",
        "    gensplit = split(gen_seq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "    sim = tally/len(seqsplit)\n",
        "    mostsim = find_similar(gen_seq)\n",
        "    input_similarity.append(sim)\n",
        "    scan_similarity.append(mostsim)\n",
        "    if iii<10:\n",
        "      print('% similarity', sim)\n",
        "      print('most similar in training set:',mostsim)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print('\\n \\n mean input similarity', mean(input_similarity))\n",
        "print('mean scanned similarity', mean(scan_similarity))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " tensor([[-0.5822, -0.3255, -1.0272,  0.5643,  0.3247,  1.8544, -2.1924, -0.2996,\n",
            "         -0.4415, -0.8155,  0.9912, -0.6549, -0.6659, -0.0915, -2.1740, -0.5521]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " GGACGAAACTCTGGAGAGACTCAAAAGAGCACCGAAGGGGAAAACTAATTTTTAGTGAAACTCTCAGGTAAAAGAACAGAGACATTTTT\n",
            ">Glycine generated: 1 \n",
            " GGACGGACCTCTGGAGAGACTCTAAAGGGCACCGAAGGGGAAAGCCAAATATTTCTGAAACTCTCAGGGTAAAAGGAAAAGGAATTTTAT\n",
            "% similarity 0.7752808988764045\n",
            "most similar in training set: 0.8\n",
            "\n",
            " tensor([[-0.7492,  1.2485,  0.2813,  0.6320,  0.4749,  0.3811,  0.4732,  0.3413,\n",
            "         -0.1413,  0.5333, -0.4301,  3.3614, -0.7986, -0.2171,  0.5082, -0.3239]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " TTCCGTTCCATGGGAGAGCGCAGATCAAGTCTGCCGCCGAAGGCGCAATTTCACCCGAAAACGCTCAGGTAAAAGGACCATGAAACCGGA\n",
            ">Glycine generated: 1 \n",
            " GCAATTTTTGCAGGAGAGAGGACCTACAAAGGTCCCGCCGAAGGAGCAACCACCCCGGAAACTCTCAGGCAAAAGGACCGAAAAAATGTA\n",
            "% similarity 0.5111111111111111\n",
            "most similar in training set: 0.8111111111111111\n",
            "\n",
            " tensor([[-1.1881,  1.1220, -0.5878,  0.6080, -0.3755, -0.3841,  0.9344,  0.4647,\n",
            "          1.2206,  0.4898,  0.2199, -0.8758,  0.0792,  0.2830,  2.3226,  3.8418]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " TGATCTGCGGCGGGAGAGTCCTGCCGGTACATTCAGCAGGCGCCGTAGGAGCAAATCCTCCCCAGGAATCTCTCAGGCCCCCGTACCGCCGCGGCGAG\n",
            ">Glycine generated: 1 \n",
            " TGATCTGCGGCGGGAGAGTCCTGCCGGTACGTTCAGCAGGCGCCGTAGGAGCAAATCCTCCCCAGGAATCTCTCAGGCCCACGTACCGCCGCGGCGAG\n",
            "% similarity 0.9795918367346939\n",
            "most similar in training set: 1.0\n",
            "\n",
            " tensor([[-1.0186, -0.0710, -2.3069, -1.5987,  1.2312,  0.6974,  0.9942,  0.5151,\n",
            "         -1.2652, -0.7908, -0.0271, -0.9851, -0.6839, -0.0066,  0.0287,  0.0131]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " CGTAAACACTCTGGAGAGAGCGGCTGCCTTCATCGGCGCAAGCCGCCGCCGAAGGGGCACGAGGCTTCCGCCTCCAAACTCTCAGGCAAAAGGACAGAGGGGCGCCC\n",
            ">Glycine generated: 1 \n",
            " CGTAAACACTCTGGAGAGACCGGCTGCATCCGTCGGCGAAAGCGCACGCAGCGCGGGCACGACGCTCCCGCCCCCAAACTCTCAGGCAAAAGGACAGAGGGGCGCCC\n",
            "% similarity 0.8598130841121495\n",
            "most similar in training set: 0.8598130841121495\n",
            "\n",
            " tensor([[ 0.4072,  0.6805, -0.4981, -0.6731,  0.7825,  0.2762, -1.0396,  0.9911,\n",
            "         -1.1591,  0.2266,  1.3088, -0.1619,  1.6250,  0.8598, -0.0099,  0.6306]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " GGTAACCACTCTGGAGAGTCTCTCTTCCATTCTGGGAAGAGAGCGCCGAAGGTGTACGGCTTACAGCCAATCTCTCAGGCATAAAGGACAGAGCATTACAT\n",
            ">Glycine generated: 1 \n",
            " GAAAGGAACTCTGGAGAGCGCCGGGTCGGTCTCGCGCCGCCAAAGGGAGAAGCGGTTTGCATACGGTCAAACTCTCAGGTAAAAGGGACAGAGGGGGGTAA\n",
            "% similarity 0.5346534653465347\n",
            "most similar in training set: 0.7326732673267327\n",
            "\n",
            " tensor([[ 1.0568,  0.4247, -0.6105,  1.2566, -1.2788,  0.8627, -0.7423,  0.3305,\n",
            "         -2.3695,  0.1165,  1.9369, -1.5544, -1.1275,  0.0881,  0.8141, -0.3328]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >ZMP input \n",
            " GATGGCTGTCGTGACTGGCGTGCAGATGGGACACCATCGGGGAACGACGTTTCGCATCAATTGGCCGCTCGCCTGGGTCGTATTT\n",
            ">ZMP generated: 1 \n",
            " TGACCGCGTCGTGACTGGCGAACAGGTGGGACACCCCCGGGGAGCGACGCGTCACCAGACTTGGCCGCCCGCCTGGGCCGCCCCGG\n",
            "% similarity 0.6823529411764706\n",
            "most similar in training set: 0.7674418604651163\n",
            "\n",
            " tensor([[ 1.6004,  0.6065, -0.1963,  2.2582,  0.4111,  0.4202,  1.6850, -1.0780,\n",
            "          0.6441,  0.0699,  0.9283, -0.8628, -0.5584, -1.1752, -0.3540,  0.3715]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " ACGATCCGGTCGGGAGAGCCTCGGCGCACGCCGAGCACCGAAGGAGCAAGCCTCCCCGCCAATCTCTCAGGTATCGCGTACCGCCCGGATCCG\n",
            ">Glycine generated: 1 \n",
            " ACGACCCCGGCGGGAGAGTCCGGGCATCGGCCCCGCACCGAAGGAGCAAACCCCCCCGTCAACCTTTCAGGTACCCGGCCCCGCGCGGCGCAG\n",
            "% similarity 0.7204301075268817\n",
            "most similar in training set: 0.7526881720430108\n",
            "\n",
            " tensor([[ 1.3104, -0.7070,  1.3981, -1.4637,  0.7015,  0.8532,  1.1148, -0.1166,\n",
            "         -1.1916,  0.6398, -0.8165,  1.7048, -0.7286,  0.1454, -1.7536, -0.7820]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " ATGATTCCAGTAGGAGAGACTCGACATCCTGAGCGCCGAAGGAGAAACTTCCTGAATGTCGCCTGTTCAGAAAGGGAAACTCTCAGGCAAAAGTACTGCTGGAAGACG\n",
            ">Glycine generated: 1 \n",
            " ATGATACTAGGGGGAGAGATCGGAGATATTCAGCCGCCGAAGGAGAAAACACTTAAAATCGCTGTTTCGGGAGGGGAAACTCTCAGGCAAAAGTACTGTTAGATGACG\n",
            "% similarity 0.6759259259259259\n",
            "most similar in training set: 0.8333333333333334\n",
            "\n",
            " tensor([[ 0.9668, -0.7554, -0.7992,  1.0276,  0.5669, -0.9859,  0.9483, -2.2054,\n",
            "         -0.0488,  1.8368, -0.1216, -0.9603, -0.7404,  0.8042,  0.7244, -0.7388]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " TCGCACGAACCGGGAGAGACCGCGCATGTCGCGGCGCCGAAGGAGCAACCGCCCCGGAAACTCTCAGGCAAAAGGACCGGTTCGCTGCA\n",
            ">Glycine generated: 1 \n",
            " GCCCCCACTGCGGGAGAGATCGGGCTTGCGCCGGCGCCGAAGGAGCAACCGCCCCGGAAACTCTCAGGCAAAAGGACCGCGGCGCGGTCA\n",
            "% similarity 0.7865168539325843\n",
            "most similar in training set: 0.8666666666666667\n",
            "\n",
            " tensor([[-0.4167,  1.3919,  2.1848,  0.2504,  0.1508,  0.2517, -1.2094, -0.9651,\n",
            "          1.9331, -1.8001,  0.4627, -0.3621, -0.4022,  1.1959,  0.5765, -1.0019]],\n",
            "       device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "\n",
            " >Glycine input \n",
            " ATGATAACAAGGGGAGAGACTACTTATGTAGCGCCGAAGGAGCAAGCAGAAATGTGAATCTCTCAGGCAAAAGAACTCTTGTAGGACG\n",
            ">Glycine generated: 1 \n",
            " ATGAAAACAAGGGGAGAGACTACTTTTGTAGCGCCGAAGGAGCAAGCATAAATGTGAATCTCTCAGGCAAAAGAACCCTTGTTGGACG\n",
            "% similarity 0.9431818181818182\n",
            "most similar in training set: 0.9431818181818182\n",
            "\n",
            " \n",
            " mean input similarity 0.7468858042924574\n",
            "mean scanned similarity 0.8366909313239939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJAqz4t-7fmL",
        "outputId": "61b0a5f2-a026-49c9-8b57-ab87833f0852"
      },
      "source": [
        "glycine1 = torch.tensor([[ 0.8636, -0.2653, -0.0748,  1.3591,  0.9868, -0.8168,  2.1340,  0.4987,\n",
        "          0.1397, -1.0721,  0.6553,  0.0429, -0.4242, -0.1992, -0.1195, -1.1274]])\n",
        "\n",
        "glycine2 = torch.tensor([[-0.9762, -0.8972, -1.9723,  2.8120,  1.2544, -0.4263,  1.0611,  0.4093,\n",
        "          1.6840,  2.6942, -1.3499,  0.3113, -1.5799,  0.4376, -1.8718,  0.3846]])\n",
        "\n",
        "dif = glycine2-glycine1\n",
        "\n",
        "steps = 5\n",
        "\n",
        "step = dif/steps\n",
        "yy = torch.tensor([[0,1,0]]).to(device).double()\n",
        "for i in range(steps+1):\n",
        "  latent_step = glycine1 + i*step\n",
        "  latent_step = latent_step.to(device).double()\n",
        "  gen = rnnvae.generate_similar(latent_step, yy)\n",
        "  gen_seq, gen_code = process_generated(gen=gen)\n",
        "  print(gen_seq)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TCGCTCGGCAGGGGGAGAGACGGCCCTGGCCCAGGCGCCAAGGGACAACCGCCCCGGAAACTCTCAGGCAAAAGGACCGCGCCGCCGAA\n",
            "TCACTCCGCACGGGGAGAGCCGGGCCTGGGCCCGCGGCCGAAGAAGCACCGGCCCGGGAACTCTCAGGCCAAAGGGCCGGCGCGCCGTCG\n",
            "TCACCCCCCACGGGGAGATCCGGCCCTGGGCCCGGGGCCGAAGAAGCAACCGCCCCGGAAACTCTCAGGCAAAAGGACCGCGCGGCGTCG\n",
            "GCACCCCCGGCGGGAGAATCCCGCCCGCGGCCGGCGGCCGAAGAAGCAACCGCCCCGGAAACCTCCCGGCCACAGGGCCGCCGGGGGCGG\n",
            "GGACCCCCCGCGGGAGAGTCCCGCCCGCGACCGGCGCCCGAAGGACCAAGCCCCCCCGGAAACTCTCAGGCCCCGCGACCCCGCGGGGGGG\n",
            "CGACCCCGCGCGGGAGAGTCCCGGGCCCGACGGGCGCCGGAAGGAGCAATTCCCCCCCGAAACTCTCACACGCCGCGGCACCGCGGGGGGGA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG5iEfd91gM7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "6d55224b-97e0-401a-9491-ac4094680174"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=l1_total, s=10)\n",
        "ax.set_title('Clustering by length')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 448x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrafWSBpwYLP"
      },
      "source": [
        "#compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C88rEHQPw-K5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "f058ae38-05f6-48bc-b087-06c24531f429"
      },
      "source": [
        "from Bio.Align import substitution_matrices\n",
        "from Bio import pairwise2\n",
        "\n",
        "blosum62 = substitution_matrices.load('BLOSUM62')\n",
        "aligner = Align.PairwiseAligner()\n",
        "aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
        "aligner.mode = 'global'\n",
        "\n",
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print('\\n >KF99 UM2020 \\n', a)\n",
        "  latent_space_rnn = rnnvae.encode(x,y)\n",
        "  latent_space_vae = vae.encode(x1)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space_rnn.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled_rnn = noise+latent_space_rnn\n",
        "    sampled_vae = noise+latent_space_vae\n",
        "    gen_rnn = rnnvae.generate_similar(sampled_rnn, y)\n",
        "    gen_vae = vae.generate_similar(sampled_vae, y)\n",
        "    gen_rnn, gen_code = process_generated(gen=gen_rnn)\n",
        "    gen_vae, gen_code1 = process_generated(gen=gen_vae)\n",
        "    rnn_score = aligner.align(a, gen_rnn)\n",
        "    rnn_score = rnn_score.score\n",
        "    vae_score = aligner.align(a, gen_vae)\n",
        "    vae_score = vae_score.score\n",
        "    print('>KF99 RNN \\n', gen_rnn)\n",
        "    print('>KF99 MLP \\n', gen_vae)\n",
        "    print('RNN score: ', rnn_score)\n",
        "    print('MLP score: ', vae_score)\n",
        "    print('% improvement: ', 100*(rnn_score-vae_score)/vae_score)\n",
        "\n",
        "      \n",
        "    alignment = pairwise2.align.localds(a, gen_rnn, blosum62, -10, -1)\n",
        "    ga1 = pairwise2.align.globalxx(a, gen_rnn)\n",
        "    print( 'ga rnn: ', ga1[0].score)\n",
        "    print(alignment[0].score)\n",
        "\n",
        "    alignment2 = pairwise2.align.localds(a, gen_vae, blosum62, -10, -1)\n",
        "    ga2 = pairwise2.align.globalxx(a, gen_vae)\n",
        "    print('ga mlp: ', ga2[0].score)\n",
        "    print(alignment2[0].score)\n",
        "\n",
        "print(latent_space)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " >KF99 UM2020 \n",
            " CGCCCTCGTGCGGGAGAGCTCCGTGACCGCCAGCCACGGACGCCGAAGGAGCAATACCTCTCCGTCAACCTCTCAGGCACCCAGGACCGCGCCAGGCCC\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-253-898b3ae991f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msampled_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlatent_space_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msampled_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlatent_space_vae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mgen_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mgen_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_vae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (8) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF1bVN7rLL-Z"
      },
      "source": [
        "\n",
        "blosum62 = substitution_matrices.load('BLOSUM62')\n",
        "aligner = Align.PairwiseAligner()\n",
        "aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
        "aligner.mode = 'global'\n",
        "\n",
        "score1rnn = []\n",
        "score1mlp = []\n",
        "score2rnn = []\n",
        "score2mlp = []\n",
        "score3rnn = []\n",
        "score3mlp = []\n",
        "\n",
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 30)\n",
        "rnnvae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  print(i/len(rnnvae_test_loader)*100, '%')\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  latent_space_rnn = rnnvae.encode(x,y)\n",
        "  latent_space_vae = vae.encode(x1)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space_rnn.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled_rnn = noise+latent_space_rnn\n",
        "    sampled_vae = noise+latent_space_vae\n",
        "    gen_rnn = rnnvae.generate_similar(sampled_rnn, y)\n",
        "    gen_vae = vae.generate_similar(sampled_vae, y)\n",
        "    gen_rnn, _ = process_generated(gen=gen_rnn)\n",
        "    gen_vae, _ = process_generated(gen=gen_vae)\n",
        "\n",
        "    rnn_score = aligner.align(a, gen_rnn)\n",
        "    rnn_score = rnn_score.score\n",
        "    score1rnn.append(rnn_score)\n",
        "    vae_score = aligner.align(a, gen_vae)\n",
        "    vae_score = vae_score.score\n",
        "    score1mlp.append(vae_score)\n",
        "\n",
        "\n",
        "\n",
        "    rnn2 = pairwise2.align.localds(a, gen_rnn, blosum62, -10, -1)\n",
        "    score2rnn.append(rnn2[0].score)\n",
        "    mlp2 = pairwise2.align.localds(a, gen_vae, blosum62, -10, -1)\n",
        "    score2mlp.append(mlp2[0].score)\n",
        "\n",
        "\n",
        "    rnn3 =  pairwise2.align.globalxx(a, gen_rnn)\n",
        "    score3rnn.append(rnn3[0].score)\n",
        "    mlp3 =  pairwise2.align.globalxx(a, gen_vae)\n",
        "    score3mlp.append(mlp3[0].score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQpBcGhSh1uG"
      },
      "source": [
        "print(mean(score1rnn), mean(score1mlp))\n",
        "print(mean(score2rnn), mean(score2mlp))\n",
        "print(mean(score3rnn), mean(score3mlp))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sluwWRNwljON"
      },
      "source": [
        "\n",
        "a,b = np.asarray(score1rnn), np.asarray(score1mlp)\n",
        "dif1 = a-b\n",
        "a,b = np.asarray(score2rnn), np.asarray(score2mlp)\n",
        "dif2 = a-b\n",
        "a,b = np.asarray(score3rnn), np.asarray(score3mlp)\n",
        "dif3 = a-b\n",
        "\n",
        "print(np.mean(dif2), np.std(dif2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkaf0pfMxUso"
      },
      "source": [
        "#Non-functional architectures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0edb2vY2bd4"
      },
      "source": [
        "## RNN-MLP vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT2dQrpf2beB"
      },
      "source": [
        "https://github.com/lyeoni/pytorch-mnist-VAE/blob/master/pytorch-mnist-VAE.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVmZmC3V2beC"
      },
      "source": [
        " class VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, hid2_dims=8, num_classes=num_classes, negative_slope=0.1):\n",
        "        super(VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.hid2_dims = hid2_dims\n",
        "        self.num_classes = num_classes\n",
        "        self.negative_slope = negative_slope\n",
        "        \n",
        "        self.gru = nn.GRU(input_size=4, hidden_size= 128, num_layers= 1, bidirectional= True, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dims, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Linear(128+256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Conditioner\n",
        "        self.conditioner = nn.Sequential(\n",
        "            nn.Linear(num_classes, 16),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(16,32),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(32, hid2_dims),\n",
        "            nn.ReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hid1_dims, 128), \n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 512), \n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        _, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h1 = torch.flatten(h, start_dim=1, end_dim = 2).contiguous()\n",
        "        # Encode input\n",
        "        # Encode input\n",
        "        x = x.reshape(x.size(0), 4*x.size(1))\n",
        "        h2 = self.encoder(x)\n",
        "        h = torch.cat((h1,h2), dim=1)\n",
        "        h = self.combine(h)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        # Encode label\n",
        "        \n",
        "        hy = self.conditioner(y)\n",
        "        # Hidden representation\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        #print('hx:', hx.shape, 'hy:', hy.shape, 'y:', y.shape)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(hx)\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hy = self.conditioner(y)\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(hx)\n",
        "        return y\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(l)\n",
        "      return(z)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      l1 = self.encoder(x)\n",
        "      mu, logvar = self.fc_mu(l1), self.fc_var(l1)\n",
        "      l2 = self._reparameterize(mu, logvar)\n",
        "      return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EwrbDpY2beC"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def vae_loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) \n",
        "    return BCE + 1.0*KLD #see Serena Yeung et. al. for KL scaling factor explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2clXWam2beC"
      },
      "source": [
        "def train_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  vae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = vae_loss_function(recon_batch, x, mu, log_var)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(vae.parameters(), 4)\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t63CtONl2beC"
      },
      "source": [
        "def test_vae(epoch, loader):\n",
        "  test_load = loader\n",
        "  vae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IApO1Ubo2beD"
      },
      "source": [
        "batch_size_vae = 100\n",
        "vae_epochs = 150\n",
        "latent_size = 16\n",
        "conditional_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpqnwJBM2beD"
      },
      "source": [
        "# build model\n",
        "vae = VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, hid2_dims = conditional_size, num_classes=num_classes, negative_slope=.01).double()\n",
        "vae = vae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(vae.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4YjL2P2beD"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msVEbbeR2beD"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_vae(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2lHqtb52beE"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  #print(y)\n",
        "  latent_space = vae.encode(x1)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "\n",
        "  if i <20:\n",
        "    print('\\n >KF99 UM2020 \\n', a) \n",
        "    for i in range(number_samples):\n",
        "      noise = torch.randn((latent_space.shape))*variability\n",
        "      noise = noise.double().to(device)\n",
        "      sampled = noise+latent_space\n",
        "      gen = vae.generate_similar(sampled, y)\n",
        "      gen_seq, gen_code = process_generated(gen=gen)\n",
        "      print('>KF99 UM202{} \\n'.format(i+1), gen_seq )\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(latent_space)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLiO70z2beE"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=l1_total, s=10)\n",
        "ax.set_title('Clustering by length')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIYlZlVwxXf"
      },
      "source": [
        "##RNNVAE w RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnC9kiKpw3nU"
      },
      "source": [
        " class RNN_VAERNN(nn.Module):\n",
        "    def __init__(self, latent_dims=128, num_encoder_layers = 2, encoder_hidden_size = 128 , num_decoder_layers=2, decoder_hidden_size = 128, seqlen = train_x.shape[0], expansion_size= 5):\n",
        "        super(RNN_VAERNN, self).__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.decoder_hidden_size = decoder_hidden_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.encoder_hidden_size = encoder_hidden_size\n",
        "        self.seqlen = seqlen\n",
        "        self.expansion_size= expansion_size\n",
        "        \n",
        "        self.gru1 = nn.GRU(input_size=4, hidden_size= self.encoder_hidden_size, num_layers= num_encoder_layers, bidirectional= True, dropout=0, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.encoder_hidden_size*self.num_encoder_layers, 256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256,128),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(128, latent_dims)\n",
        "        self.fc_var = nn.Linear(128, latent_dims)\n",
        "        \n",
        "        self.latent_to_seq= nn.Sequential(\n",
        "            nn.Linear(self.latent_dims, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, self.expansion_size*self.seqlen),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.latent_to_hidden = nn.Sequential(\n",
        "            nn.Linear(self.latent_dims, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(128, self.decoder_hidden_size*self.num_decoder_layers*2)\n",
        "        )\n",
        "        self.hidden_to_output = nn.Sequential(\n",
        "            nn.Linear(self.decoder_hidden_size*2, 256), \n",
        "            nn.LeakyReLU(),\n",
        "            #nn.Linear(256, 128),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(256, 4),\n",
        "            nn.Sigmoid())\n",
        "    \n",
        "\n",
        "        self.decoder = nn.GRU(self.expansion_size, self.decoder_hidden_size, self.num_decoder_layers, bidirectional=True, batch_first = True)\n",
        "       \n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \n",
        "        gru_out, h = self.gru1(x)\n",
        "        hp = h.permute(1,2,0)\n",
        "        hf = torch.flatten(hp, start_dim=1, end_dim = 2)\n",
        "        hx = self.encoder(hf)\n",
        "        mu, logvar = self.fc_mu(hx), self.fc_var(hx)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        lin_seq = self.latent_to_seq(hx)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(hx)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid) ###, h)\n",
        "        y = self.hidden_to_output(decoder_output)\n",
        "        return y, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "\n",
        "        lin_seq = self.latent_to_seq(y)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(y)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid)\n",
        "        y = self.hidden_to_output(decoder_output)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "\n",
        "        lin_seq = self.latent_to_seq(l)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(l)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid)\n",
        "        z = self.hidden_to_output(decoder_output)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru1(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        l2 = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.latent_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAj5162p2dY1"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def rnnvae2_loss_function(recon_x, x, mu, log_var, epo):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) #*np.sqrt(epo/vae_epochs)\n",
        "    return BCE + 0*KLD #see Serena Yeung et. al. for KL scaling factor explanation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNrlnajw2fgS"
      },
      "source": [
        "def train_rnn_vae2(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  rnnvae2.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      x = x.double().to(device)\n",
        "      recon_batch, mu, log_var = rnnvae2(x,c)\n",
        "      #print(recon_batch.shape, x.shape)\n",
        "      #recon_batch = recon_batch.permute(1,0,2).double().to(device)\n",
        "      loss = rnnvae2_loss_function(recon_batch, x, mu, log_var, epoch)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test_rnnvae2(epoch, loader):\n",
        "  test_load = loader\n",
        "  rnnvae2.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = rnnvae2(x,c)\n",
        "\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRVTFmuE5GKG"
      },
      "source": [
        "batch_size_vae = 200\n",
        "vae_epochs = 100\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "#train_index = random.sample(range(train_x.shape[1]), 200)\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vokDPjYl2l6f"
      },
      "source": [
        "# build model\n",
        "latent_size = 16\n",
        "encode_layers = 1\n",
        "encode_hsize = 128\n",
        "decode_layers = 1\n",
        "decode_hsize = 128\n",
        "expansion_size = 5\n",
        "\n",
        "rnnvae2 = RNN_VAERNN(latent_dims = latent_size, num_encoder_layers=encode_layers, encoder_hidden_size=encode_hsize, num_decoder_layers=decode_layers, decoder_hidden_size=decode_hsize, seqlen=train_x.shape[0], expansion_size=expansion_size).double()\n",
        "rnnvae2 = rnnvae2.to(device)\n",
        "optimizer_vae2 = torch.optim.Adam(rnnvae2.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc2nk1052oRa"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_rnn_vae2(epoch, optimizer_vae2, vae_train_loader)\n",
        "    test_rnnvae2(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH22f-Bun_BT"
      },
      "source": [
        "#without ReLU in pseudoseq\n",
        "#train = 173\n",
        "#test  = 172"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlzxjsYYqTDz"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae2_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae2_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print(y)\n",
        "  print(a)\n",
        "  latent_space = rnnvae2.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate(sampled)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print(gen_seq)\n",
        "\n",
        "print(latent_space)\n",
        "print(gen[0,:10,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inco12Bmlq_D"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae_test_loader = data.DataLoader(dataset_rnn(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  l = l.to('cpu')\n",
        "  x = x.double().to(device)\n",
        "  pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print('\\n >KF99 UM2020 \\n', a)\n",
        "  latent_space = rnnvae2.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print('>KF99 UM2021 \\n', gen_seq)\n",
        "\n",
        "print(latent_space)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbhZFjXtiWQq"
      },
      "source": [
        "##seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxrs9mG46kw1"
      },
      "source": [
        "def seq2seq_loss(recon_x, x, mu, log_var, epo):\n",
        "  BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "  KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())*np.sqrt(epo/seq2seq_epochs)\n",
        "  return BCE + 1*KLD #see Serena Yeung et. al. for KL scaling factor explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wzvgn6C-XCS"
      },
      "source": [
        " class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size=64, latent_size=128, num_layers = 1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size=4, hidden_size= self.hidden_size, num_layers= self.num_layers, bidirectional= True, dropout=0, batch_first = True)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.hidden_size*self.num_layers, 128),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.fc_mu = nn.Linear(128, latent_size)\n",
        "        self.fc_var = nn.Linear(128, latent_size)\n",
        "\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return hx, mu, logvar\n",
        "\n",
        "\n",
        "    def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.latent_size).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdCWP1Lw474O"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, layers, output_size = 4):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "        self.gru = nn.GRU(4, hidden_size, num_layers=self.layers, bidirectional = False, batch_first=True)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size),\n",
        "            nn.Sigmoid())\n",
        "          \n",
        "        self.softmax = nn.Softmax(dim=2) #sub for softmax?\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "\n",
        "        output = self.out(output) #output[0]? \n",
        "      \n",
        "        return output, hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lhQ_AntLQVb"
      },
      "source": [
        "dec = DecoderRNN(hidden_size=128, layers=2)\n",
        "h = torch.randn([2,100,128])\n",
        "input = torch.randn([100,19,4])\n",
        "i = input[:,0,:]\n",
        "i = i.unsqueeze(1)\n",
        "s = torch.zeros_like(input)\n",
        "for ij in range(19):\n",
        "\n",
        "  out, h = dec(i,h)\n",
        "  s[:,ij,:] = out[:,-1,:]\n",
        "  i = input[:,:ij+1,:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtllK1oxpBXZ"
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, decode_layers, lsize, hsize):\n",
        "    super(Seq2seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.latent_to_hidden = nn.Linear(lsize , hsize)\n",
        "\n",
        "  def forward(self, input, teacher_forcing):\n",
        "\n",
        "    latent, mu, log_var = self.encoder(input)\n",
        "\n",
        "    new_hidden = self.latent_to_hidden(latent)\n",
        "    #new_hidden = latent\n",
        "\n",
        "\n",
        "    tensor_length = input.size(1)\n",
        "\n",
        "    outputs = torch.zeros_like(input)\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing else False\n",
        "\n",
        "    ##decoder_input = input[:,0,:]\n",
        "    ##decoder_input = torch.unsqueeze(decoder_input, dim=1)\n",
        "    decoder_input = torch.zeros([input.size(0), 1, input.size(2)], requires_grad=True).double().to(device)\n",
        "\n",
        "    decoder_hidden = torch.stack([new_hidden for _ in range(decode_layers)], dim=0)\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(tensor_length):\n",
        "            #print('decode input:', decoder_input.shape)\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "           \n",
        "            outputs[:,di,:] = decoder_output[:,-1,:]\n",
        "            decoder_input = input[:,:di+1,:].clone()\n",
        "          \n",
        "\n",
        "            ##decoder_input = input[:,di,:]  # Teacher forcing\n",
        "            ##decoder_input = torch.unsqueeze(decoder_input, dim=1)\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(tensor_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            outputs[:,di,:] = decoder_output[:,-1,:]\n",
        "            decoder_input = outputs[:,:di+1,:].clone()\n",
        "            #decoder_input = decoder_input.detach()  # detach from history as input\n",
        "\n",
        "\n",
        "    return outputs, latent, mu, log_var\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAEMlSL_x7ly"
      },
      "source": [
        "a = torch.tensor([[3,3,3], [2,2,2]] )\n",
        "a[-1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg6dHqPRr29Q"
      },
      "source": [
        "def train_seq2seq1(model, loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "\n",
        "  for i, (x,y,l) in enumerate(loader):\n",
        "    optimizer.zero_grad()\n",
        "    x = x.double().to(device)\n",
        "    reconstructed, latent, mu, log_var = model(x, 1-.5*np.sqrt(epoch/seq2seq_epochs))\n",
        "    loss = seq2seq_loss(reconstructed, x, mu, log_var, epoch)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "\n",
        "  print('\\n ====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(loader.dataset)))\n",
        "  return reconstructed, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvQq3L0xxF1"
      },
      "source": [
        "def evaluate(model, loader, epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (x,y,l) in enumerate(loader):\n",
        "            x = x.double().to(device)\n",
        "\n",
        "            output, latent, mu, logvar = model(x, 0) #turn off teacher forcing\n",
        "\n",
        "            loss = seq2seq_loss(output, x, mu, logvar, epo=0)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        print(' ====> Average test loss:     {:.4f}'.format( epoch_loss / len(loader.dataset)))\n",
        "    return output, x, epoch_loss / len(loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aICt2H7bxR1z"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4MqeY8Gys93"
      },
      "source": [
        "a = torch.randn([3,5])\n",
        "b = torch.zeros_like(a)\n",
        "for i in range(a.size(0)):\n",
        "  b[i,:] = a[i,:]\n",
        "\n",
        "print(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjMDHy6JJbks"
      },
      "source": [
        "batch_size_vae = 4\n",
        "seq2seq_epochs = 50\n",
        "#train_index = range(len(train_x[0,:,0]))\n",
        "train_index = random.sample(range(train_x.shape[1]), 200)\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICLxFO_0rmkR"
      },
      "source": [
        "latent_size = 3\n",
        "encode_hidden_size = 128\n",
        "decode_hidden_size = 256\n",
        "decode_layers = 2\n",
        "encoder = EncoderRNN(hidden_size = encode_hidden_size, latent_size=latent_size, num_layers = 2).double().to(device)\n",
        "decoder = DecoderRNN(hidden_size=decode_hidden_size, layers = decode_layers).double().to(device)\n",
        "seq2seq = Seq2seq(encoder=encoder, decoder = decoder, decode_layers=decode_layers, lsize = latent_size, hsize = decode_hidden_size).double().to(device)\n",
        "seq2seq_optimizer = torch.optim.Adam(seq2seq.parameters(), lr=.001)\n",
        "seq2seq.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9oO63mtKfw"
      },
      "source": [
        "with torch.autograd.set_detect_anomaly(True):\n",
        "  for epoch in range(seq2seq_epochs):\n",
        "      out = train_seq2seq1(seq2seq, vae_train_loader, seq2seq_optimizer, epoch)\n",
        "      _,__,test_loss = evaluate(seq2seq, vae_test_loader, epoch)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tsJU62UQvRE"
      },
      "source": [
        "pred = out[0]\n",
        "tar = out[1]\n",
        "print(pred[0, :10,:])\n",
        "print(tar[0, :10  ,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_WjZr5pj43G"
      },
      "source": [
        "\n",
        "gen_seq, gen_code = process_generated(gen=pred[1])\n",
        "seq, _ = process_generated(gen=tar[1])\n",
        "print(gen_seq)\n",
        "print(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEbCgDjgjysc"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae2_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:]), batch_size = 1)\n",
        "for i, (x,y) in enumerate(rnnvae2_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print(y)\n",
        "  print(a)\n",
        "  latent_space = EncoderRNN.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print(gen_seq)\n",
        "\n",
        "print(latent_space)\n",
        "print(gen[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mTN7h-8Pzkk"
      },
      "source": [
        "enc = EncoderRNN(hid1_dims=128)\n",
        "enc._init_weights()\n",
        "a = torch.randn([10, 110, 4])\n",
        "out, mu, log = enc(a)\n",
        "dec = DecoderRNN(hidden_size=128)\n",
        "hid = torch.unsqueeze(out, dim=0)\n",
        "ran = torch.randn([10,1,4])\n",
        "loss = 0\n",
        "criter = nn.MSELoss()\n",
        "for i in range(30):\n",
        "  outp, hid = dec(ran, hid)\n",
        "  if i == 0:\n",
        "    print(outp)\n",
        "  else:\n",
        "    ran = outp.detach()\n",
        "    tar = a[:,i,:]\n",
        "    loss += criter(tar.flatten(), ran.flatten())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4p39Rh9rn0E"
      },
      "source": [
        "#1. set up for loop to create many synthetic sequences\n",
        "#2. convert synthetic sequences to binary and append to real sequences\n",
        "#3. create new train_loader to incorporate new data\n",
        "#4. train new RNN on new data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMfPPvsYUtMZ"
      },
      "source": [
        "\n",
        "generated_list = []\n",
        "generated_id = []\n",
        "synthetic = []\n",
        "for ij in range(num_classes):\n",
        "  print('NEW TYPE')\n",
        "  gen_type = ij+1\n",
        "  a = np.array(gen_type).reshape(-1,1)\n",
        "  b = enc.transform(a).toarray()\n",
        "  c = torch.tensor(b).double().to(device)\n",
        "  avg_len = []\n",
        "  for ii in range(number_samples):\n",
        "    sample = model.generate(c)\n",
        "\n",
        "    test = sample.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "    test = np.asarray(test.detach())\n",
        "\n",
        "    pred_binary = np.zeros_like(test)\n",
        "    pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "    a = np.sum(test, 1)\n",
        "\n",
        "    #plt.plot(a)\n",
        "    #plt.show()\n",
        "\n",
        "    stop_point = []\n",
        "    for i, j in enumerate(a):\n",
        "      if j <=.1:\n",
        "        stop_point.append(i)\n",
        "\n",
        "    #print(stop_point[0:4])\n",
        "    if not stop_point:\n",
        "      pass\n",
        "    elif stop_point[0] <= 40:\n",
        "      pass\n",
        "    else:\n",
        "      pred_binary = pred_binary[:stop_point[0],:]\n",
        "      avg_len.append(stop_point[0])\n",
        "\n",
        "      s = []\n",
        "      for i in pred_binary:\n",
        "        if i[0] == 1:\n",
        "          s.append('A')\n",
        "        elif i[1] == 1:\n",
        "          s.append('C')\n",
        "        elif i[2] == 1:\n",
        "          s.append('G')\n",
        "        elif i[3] == 1:\n",
        "          s.append('T')\n",
        "\n",
        "      result = ''\n",
        "      for element in s:\n",
        "          result += str(element)\n",
        "\n",
        "      generated = [result]\n",
        "\n",
        "      print(generated)\n",
        "      synthetic.append(generated)\n",
        "      seq, _ = sequence_one_hot(generated)\n",
        "      #print(seq.size(0))\n",
        "      zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "      generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "      g = generated_sequence.permute(1,0,2)\n",
        "      g = g.to(device)\n",
        "      generated_list.append(g)\n",
        "\n",
        "      generated_id.append(c)\n",
        "\n",
        "  # print(len(generated_list))\n",
        "  # print(mean(avg_len))\n",
        "a = generated_list[0]\n",
        "for i in range(1,len(generated_list)):\n",
        "  a = torch.cat((a,generated_list[i]))\n",
        "\n",
        "b = a.permute(1,0,2)\n",
        "synthetic = b.double().to(device)\n",
        "\n",
        "train_mixed = torch.cat((train_x,synthetic), dim=1)\n",
        "\n",
        "#print(train_mixed.shape)\n",
        "\n",
        "a = generated_id[0]\n",
        "for i in range(1,len(generated_id)):\n",
        "  a = torch.cat((a, generated_id[i]))\n",
        "\n",
        "mixed_y = torch.cat((train_y, a))\n",
        "\n",
        "return train_mixed, mixed_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZsKqRMbw-TI"
      },
      "source": [
        "def generate_synthetic(number_samples, model):\n",
        "  generated_list = []\n",
        "  generated_id = []\n",
        "  synthetic = []\n",
        "  for ij in range(num_classes):\n",
        "    print('NEW TYPE')\n",
        "    gen_type = ij+1\n",
        "    a = np.array(gen_type).reshape(-1,1)\n",
        "    b = enc.transform(a).toarray()\n",
        "    c = torch.tensor(b).double().to(device)\n",
        "    avg_len = []\n",
        "    for ii in range(number_samples):\n",
        "      sample = model.generate(c)\n",
        "\n",
        "      test = sample.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "      test = np.asarray(test.detach())\n",
        "\n",
        "      pred_binary = np.zeros_like(test)\n",
        "      pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "      a = np.sum(test, 1)\n",
        "\n",
        "      #plt.plot(a)\n",
        "      #plt.show()\n",
        "\n",
        "      stop_point = []\n",
        "      for i, j in enumerate(a):\n",
        "        if j <=.1:\n",
        "          stop_point.append(i)\n",
        "\n",
        "      #print(stop_point[0:4])\n",
        "      if not stop_point:\n",
        "        pass\n",
        "      elif stop_point[0] <= 40:\n",
        "        pass\n",
        "      else:\n",
        "        pred_binary = pred_binary[:stop_point[0],:]\n",
        "        avg_len.append(stop_point[0])\n",
        "\n",
        "        s = []\n",
        "        for i in pred_binary:\n",
        "          if i[0] == 1:\n",
        "            s.append('A')\n",
        "          elif i[1] == 1:\n",
        "            s.append('C')\n",
        "          elif i[2] == 1:\n",
        "            s.append('G')\n",
        "          elif i[3] == 1:\n",
        "            s.append('T')\n",
        "\n",
        "        result = ''\n",
        "        for element in s:\n",
        "            result += str(element)\n",
        "\n",
        "        generated = [result]\n",
        "\n",
        "        print(generated)\n",
        "        synthetic.append(generated)\n",
        "        seq, _ = sequence_one_hot(generated)\n",
        "        #print(seq.size(0))\n",
        "        zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "        generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "        g = generated_sequence.permute(1,0,2)\n",
        "        g = g.to(device)\n",
        "        generated_list.append(g)\n",
        "\n",
        "        generated_id.append(c)\n",
        "\n",
        "   # print(len(generated_list))\n",
        "   # print(mean(avg_len))\n",
        "  a = generated_list[0]\n",
        "  for i in range(1,len(generated_list)):\n",
        "    a = torch.cat((a,generated_list[i]))\n",
        "\n",
        "  b = a.permute(1,0,2)\n",
        "  synthetic = b.double().to(device)\n",
        "\n",
        "  train_mixed = torch.cat((train_x,synthetic), dim=1)\n",
        "\n",
        "  #print(train_mixed.shape)\n",
        "\n",
        "  a = generated_id[0]\n",
        "  for i in range(1,len(generated_id)):\n",
        "    a = torch.cat((a, generated_id[i]))\n",
        "\n",
        "  mixed_y = torch.cat((train_y, a))\n",
        "\n",
        "  return train_mixed, mixed_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMwt-qh5DzlS"
      },
      "source": [
        "train_mixed, mixed_y = generate_synthetic(100, vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJX8nKNG6WT2"
      },
      "source": [
        "learn_rate_rnn = .001\n",
        "rnn_epochs = 100\n",
        "rnn_batch_size = 200\n",
        "rnn_dropout_rate = .3\n",
        "rnn_layers = 1\n",
        "rnn_width = 128\n",
        "dense_width = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kocmd_j6b0l"
      },
      "source": [
        "\n",
        "rnn1 = RNN(drop_p = rnn_dropout_rate, num_layers=rnn_layers, h_rnn = rnn_width, fc_layer = dense_width, batch_size=rnn_batch_size).to(device)\n",
        "rnn1 = rnn1.double()\n",
        "rnn_optimizer = torch.optim.Adam(rnn1.parameters(), lr=learn_rate_rnn)\n",
        "\n",
        "rnn_train_index = range(len(train_x[0,:,0]))\n",
        "rnn_test_index = range(len(test_x[0,:100,0]))\n",
        "print('new_fold')\n",
        "\n",
        "train_loader = data.DataLoader(dataset_rnn(train_x[:,rnn_train_index,:], train_y[rnn_train_index,:]), batch_size=rnn_batch_size)\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=rnn_batch_size)\n",
        "\n",
        "for epoch in range(rnn_epochs):\n",
        "  train_losses = train(device, rnn_optimizer, train_loader, rnn1)\n",
        "  test_loss, y_pred_test1, y_test1= test_rnn(device, test_loader, rnn1)\n",
        "  avg_train_loss = sum(train_losses)/len(train_losses)\n",
        "  print('epoch:', epoch, 'loss:', avg_train_loss, \"test:\", test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuIT57KDB3N"
      },
      "source": [
        "\n",
        "rnn2 = RNN2(drop_p = rnn_dropout_rate, num_layers=rnn_layers, h_rnn = rnn_width, fc_layer = dense_width, batch_size=rnn_batch_size).to(device)\n",
        "rnn2 = rnn2.double()\n",
        "rnn_optimizer2 = torch.optim.Adam(rnn2.parameters(), lr=learn_rate_rnn)\n",
        "\n",
        "rnn_train_index2 = range(len(train_x[0,:,0]))\n",
        "rnn_test_index = range(len(train_mixed[0,:100,0]))\n",
        "print('new_fold')\n",
        "\n",
        "train_loader = data.DataLoader(dataset_rnn(train_mixed[:,rnn_train_index2,:], mixed_y[rnn_train_index2,:]), batch_size=rnn_batch_size)\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=rnn_batch_size)\n",
        "\n",
        "for epoch in range(rnn_epochs):\n",
        "  train_losses = train(device, rnn_optimizer2, train_loader, rnn2)\n",
        "  test_loss, y_pred_test2, y_test2 = test_rnn(device, test_loader, rnn2) \n",
        "  avg_train_loss = sum(train_losses)/len(train_losses)\n",
        "  print('epoch:', epoch, 'loss:', avg_train_loss, \"test:\", test_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkcI5GOEk4vP"
      },
      "source": [
        "rnn_test_index = range(test_x.shape[1]) #test_x.shape[1])\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=1000)\n",
        "_, y_pred_test2, y_test2= test_rnn(device, test_loader, rnn2) \n",
        "_, y_pred_test1, y_test1= test_rnn(device, test_loader, rnn1) \n",
        "\n",
        "\n",
        "p1, matrix1, avg_auc1, f11 = generate_report(y_pred_test1, y_test1)\n",
        "p1.show()\n",
        "print(avg_auc1)\n",
        "print(matrix1)\n",
        "p2, matrix2, avg_auc2, f12 = generate_report(y_pred_test2, y_test2)\n",
        "p2.show()\n",
        "print(avg_auc2)\n",
        "print(matrix2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}