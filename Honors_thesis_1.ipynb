{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Honors_thesis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "24aVbFf1qrK8",
        "E0edb2vY2bd4"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kfin2343/riboswitches/blob/main/Honors_thesis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAwxIBBwBkdk"
      },
      "source": [
        "#function setup & data pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akVCi9MXL_fs"
      },
      "source": [
        "!pip install biopython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjW7iDA_BhQE"
      },
      "source": [
        "import os, requests, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.cm as cm\n",
        "import statistics\n",
        "from statistics import mean\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from Bio import Align\n",
        "from Bio.Align import substitution_matrices\n",
        "from Bio import pairwise2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izC3ae4qBpbH"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLWwtHE6_iiV"
      },
      "source": [
        "##### REMOVE OUTLIERS BY TAKING SWITCHES OF LENGTH 50 TO 110 #######\n",
        "num =  80\n",
        "var = 50\n",
        "#### IMPORT FAMILIES \n",
        "#sets = ['Fluoride', 'Glutamine', 'Glycine', 'Purine', 'ZMP']\n",
        "#sets = ['Fluoride', 'Glutamine', 'Glycine', 'Purine', 'ZMP', 'FMN', 'SAMIIV', 'TPP', 'nhaA']\n",
        "#sets = ['Glutamine', 'Glycine', 'ZMP']\n",
        "sets = ['Glycine']\n",
        "num_classes = 1\n",
        "fam_len = []\n",
        "fam_sequences = []\n",
        "lll = []\n",
        "for u in sets:\n",
        "  url = 'https://raw.githubusercontent.com/Kfin2343/riboswitches/main/{}_riboswitch.csv'.format(u)\n",
        "  s=requests.get(url).content\n",
        "  df=pd.read_csv(io.StringIO(s.decode('utf-8')), sep = \",\") #pandas datatframe\n",
        "  sequence1 = np.asarray(df['0'])\n",
        "  exact_len = []\n",
        "  ll = []\n",
        "  for i in range(len(sequence1)): \n",
        "    l = len(sequence1[i])\n",
        "    ll.append(l)\n",
        "    if num-var< l < num+var: #only accept lengths between 50 and 110\n",
        "      a = sequence1[i]\n",
        "      exact_len.append(a)\n",
        "  sequence = np.asarray(exact_len) #convert to numpy array\n",
        "  sequence = np.expand_dims(sequence, 1) #2d array\n",
        "  fam_len.append(len(sequence))  #class size, used for label creation\n",
        "  fam_sequences.append(sequence) \n",
        "  lll.append(mean(ll))\n",
        "ex_seq = fam_sequences[0] #needed for concatenating along new dim (below)\n",
        "for i in range(1,len(fam_len)):\n",
        "  ex_seq = np.concatenate((ex_seq, fam_sequences[i]))\n",
        "ex_seq = np.squeeze(ex_seq)\n",
        "\n",
        "###create classification label vector\n",
        "fam_idx = [] \n",
        "for i in range(1, len(fam_len)+1):\n",
        "  a = np.empty((fam_len[i-1], 1))\n",
        "  a.fill(i)\n",
        "  fam_idx.append(a)\n",
        "fam_id = np.concatenate((fam_idx), axis = 0)\n",
        "\n",
        "###convert label to one hot encoded vector\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(fam_id) \n",
        "family_id = enc.transform(fam_id).toarray()\n",
        "family_id = torch.tensor(family_id).double().to(device)\n",
        "\n",
        "print(lll)\n",
        "for i in range(num_classes):\n",
        "  print(len(fam_sequences[i])) #family counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gX5wCdwBry5"
      },
      "source": [
        "def sequence_one_hot(sequence_array):\n",
        " \n",
        " \n",
        "  seq_list = []\n",
        "  \n",
        "  for a in sequence_array:\n",
        "    split = []\n",
        "    split = [char for char in a]\n",
        "    seq_list.append(split)\n",
        "\n",
        "  resnames = ['G', 'T', 'A', 'C']\n",
        "    \n",
        "  label_encoder = LabelEncoder()\n",
        "  encode_names = label_encoder.fit_transform(resnames)\n",
        "  code = dict(zip(resnames, encode_names))\n",
        "\n",
        "  sequences = []\n",
        "  for i in range(len(seq_list)):\n",
        "    position_sequence = []\n",
        "    for r in seq_list[i]:\n",
        "      position = code[r]\n",
        "      position_sequence.append(position)\n",
        "    \n",
        "    values = np.asarray(position_sequence).reshape(len(position_sequence), 1)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False) #build encoder\n",
        "    enc = encode_names.reshape(len(encode_names),1) #reshape into two dimensions before fitting\n",
        "    onehot_encoder.fit(enc)  #assign constant 'code' for one hot encoding from 'resnames' list above\n",
        "    encoded_matrix = onehot_encoder.transform(values) #convert to [x,20] dimensions, one hot encoded\n",
        "    sequences.append(encoded_matrix)\n",
        "\n",
        "  lx = []\n",
        "  lenx = []\n",
        "\n",
        "  for a in sequences: \n",
        "      tensor = torch.tensor(a) #convert to tensor\n",
        "      lx.append(tensor) #append to list\n",
        "      lenx.append(a.shape[0]) #need length for pack_padded_sequence\n",
        "\n",
        "  padded_sequence = pad_sequence(lx, padding_value=0)\n",
        "\n",
        "  return padded_sequence, lenx\n",
        "\n",
        "padded_sequence, lengths = sequence_one_hot(sequence_array = ex_seq) #processed data\n",
        "padded_sequence = padded_sequence.double() #dtype\n",
        "padded_sequence = padded_sequence.to(device) #cast to gpu\n",
        "lengths = torch.tensor(lengths).double().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvCuu5wtpDY"
      },
      "source": [
        "def find_similar(seq):\n",
        "  similarity_list = []\n",
        "  seqsplit = split(seq)\n",
        "  for ii in range(len(ex_seq)):  \n",
        "    iq = ex_seq[ii]\n",
        "    gensplit = split(iq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "    a = tally/len(seqsplit)\n",
        "    similarity_list.append(a)\n",
        "\n",
        "  return max(similarity_list)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wZb1V_QEuXZ"
      },
      "source": [
        "\n",
        "class dataset_rnn(data.Dataset):\n",
        "  def __init__(self, x, y, l):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.l = l\n",
        "  def __len__(self):\n",
        "    return len(self.x[0,:,0])\n",
        "  def __getitem__(self, index):\n",
        "    X = self.x[:, index, :]\n",
        "    Y = self.y[index,:]\n",
        "    L = self.l[index]\n",
        "    return X, Y, L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28bIumCHEyxT"
      },
      "source": [
        "class dataset(data.Dataset):\n",
        "  def __init__(self, x, t, l):\n",
        "    self.x = x\n",
        "    self.t = t\n",
        "    self.l = l\n",
        "  def __len__(self):\n",
        "    return len(self.x[0,:,0])\n",
        "  def __getitem__(self, index):\n",
        "    X = self.x[:, index, :]\n",
        "    T = self.t[index, :]\n",
        "    L = self.l[index]\n",
        "    return X, T, L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlv2GXu8cMey"
      },
      "source": [
        "all_index = np.arange(0, len(family_id[:,0]))\n",
        "train_index = []\n",
        "fcs = np.cumsum(fam_len)\n",
        "for i in range(num_classes):\n",
        "  if i == 0:\n",
        "    ix = random.sample(range(0, fcs[0]), 9*len(fam_sequences[i])//10)\n",
        "    train_index.append(ix)\n",
        "  else:\n",
        "    ix = random.sample(range(fcs[i-1], fcs[i]), 9*len(fam_sequences[i])//10)\n",
        "    train_index.append(ix)\n",
        "    \n",
        "a = []\n",
        "for i in train_index:\n",
        "  for x in i:\n",
        "    a.append(x)\n",
        "\n",
        "train_index = np.asarray(a).flatten()\n",
        "rng = np.random.default_rng()\n",
        "rng.shuffle(train_index)\n",
        "print(train_index.shape)\n",
        "test_index = np.delete(all_index, train_index)\n",
        "\n",
        "\n",
        "train_index = torch.tensor(train_index).to(device)\n",
        "valid_index = torch.tensor(test_index).to(device)\n",
        "\n",
        "print(train_index.shape, test_index.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kA2AQK70nJn"
      },
      "source": [
        "train_x = padded_sequence[:,train_index,:]\n",
        "train_length = lengths[train_index]\n",
        "test_length = lengths[valid_index]\n",
        "test_x = padded_sequence[:,valid_index,:]\n",
        "train_y = family_id[train_index,:]\n",
        "test_y = family_id[valid_index,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPGZe7H4vpWE"
      },
      "source": [
        "def generate_report(predicted, ground_truth):\n",
        "\n",
        "  predicted = predicted.reshape(predicted.size(0), num_classes).to('cpu')\n",
        "  predicted = np.asarray(predicted.detach())\n",
        "  pred_binary = np.zeros_like(predicted)\n",
        "  pred_binary[np.arange(len(predicted)), predicted.argmax(1)] = 1\n",
        "  ground_truth = np.asarray(ground_truth.to('cpu'))\n",
        "  matrix = classification_report(ground_truth, pred_binary, digits=4)\n",
        "  f1 = f1_score(ground_truth, pred_binary, average='macro')\n",
        "  targets = []\n",
        "  for i in range (num_classes):\n",
        "    a = str(i)\n",
        "    targets.append(a)\n",
        "  ##targets = ['1', '2', '3', '4', '5']\n",
        "  colors = cm.rainbow(np.linspace(0, 1, len(targets)))\n",
        "  plt.figure()\n",
        "  lw = 3\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "\n",
        "  for i, target in enumerate(targets):\n",
        "    fpr[i], tpr[i], thresholds = roc_curve(ground_truth[:,i], predicted[:,i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                        ''.format(i, roc_auc[i]))  # roc_auc_score\n",
        "\n",
        "  avg_score = mean(roc_auc[k] for k in roc_auc)\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=(1.1,0))\n",
        "  #plt.show()\n",
        "\n",
        "  return plt, matrix, avg_score, f1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB3tW-WXyWDV"
      },
      "source": [
        "def split(word): \n",
        "    return list(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5UOZkeAsrol"
      },
      "source": [
        "def process_generated(gen):\n",
        "  test = gen.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "  test = np.asarray(test.detach())\n",
        "\n",
        "  pred_binary = np.zeros_like(test)\n",
        "  pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "  a = np.sum(test, 1)\n",
        "\n",
        "  #plt.plot(a)\n",
        "  #plt.show()\n",
        "\n",
        "  stop_point = []\n",
        "  for i, j in enumerate(a):\n",
        "    if j <=.2:\n",
        "      stop_point.append(i)\n",
        "\n",
        "  #print(stop_point[0:4])\n",
        "  if not stop_point:\n",
        "    pass\n",
        "  elif stop_point[0] <= 40:\n",
        "    pass\n",
        "  else:\n",
        "    pred_binary = pred_binary[:stop_point[0],:]\n",
        "\n",
        "  s = []\n",
        "  for i in pred_binary:\n",
        "    if i[0] == 1:\n",
        "      s.append('A')\n",
        "    elif i[1] == 1:\n",
        "      s.append('C')\n",
        "    elif i[2] == 1:\n",
        "      s.append('G')\n",
        "    elif i[3] == 1:\n",
        "      s.append('T')\n",
        "\n",
        "  result = ''\n",
        "  for element in s:\n",
        "      result += str(element)\n",
        "\n",
        "  generated = [result]\n",
        "\n",
        "  seq, _ = sequence_one_hot(generated)\n",
        "  #print(seq.size(0))\n",
        "  zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "  generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "  g = generated_sequence.permute(1,0,2)\n",
        "  g = g.to(device)\n",
        "  \n",
        "  return result, g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGC0589lJZvn"
      },
      "source": [
        "def cyclic_anneal(epoch, epochs, num_cycles):\n",
        "  cycle_length = epochs//num_cycles\n",
        "  #seperate first x cycles and last cycle\n",
        "  cycle_list = []\n",
        "  for i in range(1,num_cycles+1):\n",
        "    if i == num_cycles:\n",
        "      cycle_list.append(epochs)\n",
        "    else:\n",
        "      a = cycle_length*i\n",
        "      cycle_list.append(a)\n",
        "\n",
        "  if epoch <= cycle_list[0]:\n",
        "    if epoch < cycle_length //2:\n",
        "      beta = epoch/(cycle_length//2)\n",
        "    else:\n",
        "      beta = 1\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  for ii in cycle_list:\n",
        "    r = epoch/ii\n",
        "    if 1<r<=2:\n",
        "      a = epoch-ii\n",
        "      if a < cycle_length //2:\n",
        "        beta = a/(cycle_length//2)\n",
        "      else:\n",
        "        beta = 1\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return beta\n",
        "\n",
        "for i in range(100):\n",
        "  a = cyclic_anneal(i, 100, 3)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfsYCIhJEFG"
      },
      "source": [
        "# vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fxqQfZRy1_"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "midiuuF7GYFg"
      },
      "source": [
        " class VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, hid2_dims=8, num_classes=num_classes, negative_slope=0.1):\n",
        "        super(VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.hid2_dims = hid2_dims\n",
        "        self.num_classes = num_classes\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dims+num_classes, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Conditioner\n",
        "        self.conditioner = nn.Sequential(\n",
        "            nn.Linear(num_classes, 16),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(16,32),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(32, hid2_dims),\n",
        "            nn.ReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hid1_dims+num_classes, 128), \n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 512), \n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x,y], dim=1)\n",
        "        # Encode input\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        # Encode label\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(h)\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hy = self.conditioner(y)\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(h)\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(h)\n",
        "      return(z)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      x = torch.cat([x,y], dim=1)\n",
        "      l1 = self.encoder(x)\n",
        "      mu, logvar = self.fc_mu(l1), self.fc_var(l1)\n",
        "      l2 = self._reparameterize(mu, logvar)\n",
        "      return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "        x = torch.cat([x,y], dim=1)\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8_2fkAWR4pH"
      },
      "source": [
        "##VAE loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMLa_g__Jnr6"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def vae_loss_function(recon_x, x, mu, log_var, epoch,cycles):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = (-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())-5)*cyclic_anneal(epoch, vae_epochs, cycles) \n",
        "    return BCE + 5*KLD, KLD/(KLD+BCE)  #see Serena Yeung et. al. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQLgBcZR9O3"
      },
      "source": [
        "##train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ll52wVUbPP"
      },
      "source": [
        "\n",
        "def train_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  vae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss, KL = vae_loss_function(recon_batch, x, mu, log_var, epoch,cycles)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(vae.parameters(), 4)\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)), 'KL:', KL.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vjsBG_T_Zqp"
      },
      "source": [
        "def test_vae(epoch, loader):\n",
        "  test_load = loader\n",
        "  vae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(test_load):\n",
        "      x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTVO-LIkSAEx"
      },
      "source": [
        "##hyperparam, build, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-twHyYyN63e"
      },
      "source": [
        "batch_size_vae = 100\n",
        "vae_epochs = 100\n",
        "latent_size = 16\n",
        "conditional_size = 32 ##ignore\n",
        "cycles = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3vGo4HkJpRr"
      },
      "source": [
        "# build model\n",
        "vae = VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, hid2_dims = conditional_size, num_classes=num_classes, negative_slope=.01).double()\n",
        "vae = vae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(vae.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfJAhQTQEih7"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy_NzpjXK-fd"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_vae(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378VcPlbSF26"
      },
      "source": [
        "##eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXLXrsEQs1jj"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(train_index)), 300)\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "ids = []\n",
        "G_content = []\n",
        "scan_similarity = []\n",
        "input_similarity = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "l_di = dict(zip(ids, sets))\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "for iii, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  seq,_ = process_generated(x1)\n",
        "  seqsplit = split(seq)\n",
        "  Gcontent = 0\n",
        "  for i in range(len(seqsplit)):\n",
        "    a = seqsplit[i]\n",
        "    if a == 'T':\n",
        "      Gcontent +=1\n",
        "  G_content.append(Gcontent)\n",
        "  #print(y)\n",
        "  latent_space = vae.encode(x1)\n",
        "  ##print('\\n', latent_space)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "  yi = int(np.asarray(y.to('cpu')).argmax(1))\n",
        "  yid = l_di[yi]\n",
        "  if iii <10:\n",
        "    print('\\n >{} input \\n'.format(yid), seq) \n",
        "  \n",
        "  for ii in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = vae.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    if iii<10:\n",
        "      print('>{}{}_generated \\n'.format(yid, ii+1), gen_seq)\n",
        "    else:\n",
        "      pass\n",
        "    seqsplit = split(seq)\n",
        "    gensplit = split(gen_seq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    sim = tally/len(seqsplit)\n",
        "    mostsim = find_similar(gen_seq)\n",
        "    input_similarity.append(sim)\n",
        "    scan_similarity.append(mostsim)\n",
        "\n",
        "    if iii<10:\n",
        "      q = iii\n",
        "      print('% similarity', sim)\n",
        "      print('most similar in training set:',mostsim)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "print('\\n \\n mean input similarity', mean(input_similarity))\n",
        "print('mean scanned similarity', mean(scan_similarity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b7SwGNOk5Mv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rt62PJH2xuy"
      },
      "source": [
        "glycine1 =  torch.tensor([[ 0.8786, -2.5213,  1.3548,  0.4012, -0.8996, -1.0660, -0.1862,  1.4701,\n",
        "          0.0578, -1.3307,  1.1065,  1.7855, -0.1441,  0.4214,  1.1755,  0.0341]])\n",
        "glycine2 =  torch.tensor([[ 1.5450,  1.2961,  1.7923,  0.3352,  0.6877, -0.4763,  0.5722, -0.3662,\n",
        "         -1.3082, -0.0468, -2.6488, -1.4083, -0.1070, -0.9566, -0.1814,  1.7470]])\n",
        "dif = glycine2-glycine1\n",
        "\n",
        "steps = 5\n",
        "\n",
        "step = dif/steps\n",
        "yy = torch.tensor([[0,1,0]]).to(device).double()\n",
        "for i in range(steps+1):\n",
        "  latent_step = glycine1 + i*step\n",
        "  latent_step = latent_step.to(device).double()\n",
        "  gen = vae.generate_similar(latent_step, yy)\n",
        "  gen_seq, gen_code = process_generated(gen=gen)\n",
        "  print(gen_seq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kBsW8wKOwPR"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "G_total = G_content[0]\n",
        "for i in range(1,len(G_content)):\n",
        "  g = G_content[i]\n",
        " # G_total = np.concatenate((G_total, g), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=G_content, s=10)\n",
        "ax.set_title('Clustering by U Content')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkxxrQO8ODYy"
      },
      "source": [
        "# rnn vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwHxvIEyySCi"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7fqS7wxODYz"
      },
      "source": [
        " class RNN_VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, embedsize=8, num_classes=num_classes, hsize=256, num_layers = 1):\n",
        "        super(RNN_VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.embedsize = embedsize\n",
        "        self.num_classes = num_classes\n",
        "        self.drop_p = .3\n",
        "        self.num_layers = num_layers\n",
        "        self.hsize = hsize\n",
        "        \n",
        "        self.embed = nn.Embedding(4,self.embedsize)\n",
        "\n",
        "        self.gru = nn.GRU(input_size=self.embedsize, hidden_size= self.hsize, num_layers= num_layers, bidirectional= True, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.hsize*self.num_layers, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(128)\n",
        "        )\n",
        "\n",
        "        self.transform = nn.Sequential(nn.LeakyReLU())\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.hid1_dims+num_classes, 256), \n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        ind = x.argmax(2).type(torch.LongTensor)\n",
        "        ind = ind.to(device)\n",
        "        x = self.embed(ind)\n",
        "        x = x.double()\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2).contiguous()\n",
        "        # Encode input\n",
        "        #h = torch.cat([h,y], dim=1)\n",
        "        h2 = self.encoder(h)\n",
        "        #h = self.transform(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2) \n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(h) ####h\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(h) ####h\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(h) ####z\n",
        "      return z \n",
        "\n",
        "\n",
        "    def encode(self, x,y):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        ind = x.argmax(2).type(torch.LongTensor)\n",
        "        ind = ind.to(device)\n",
        "        x = self.embed(ind)\n",
        "        x = x.double()\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "       # h = torch.cat([h,y], dim=1)\n",
        "        h2 = self.encoder(h)\n",
        "        h = self.transform(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        l2 = self._reparameterize(mu, logvar)\n",
        "        return l2\n",
        "\n",
        " #   def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "  #      gru_out, h = self.gru(x) ##, self.hidden)\n",
        "   #     h = h.permute(1,2,0)\n",
        "    #    h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "     #   h2 = self.encoder(h)\n",
        "      #  mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "       # hx = self._reparameterize(mu, logvar)\n",
        "        #return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwDLmn22w60S"
      },
      "source": [
        "##RNNVAE loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kTWCcUnODY2"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def rnnvae_loss_function(recon_x, x, mu, log_var, epo):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())*cyclic_anneal(epoch, vae_epochs, cycles)\n",
        "    return BCE + 1.5*KLD #see Serena Yeung et. al. for KL scaling factor explanation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MaXNX3Aw-1Z"
      },
      "source": [
        "##train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqK7hbb-ODY8"
      },
      "source": [
        "def train_rnn_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  rnnvae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(train_loader):\n",
        "      x = x.double().to(device)\n",
        "      l = l.to('cpu')\n",
        "      pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = rnnvae(x,c)\n",
        "      loss = rnnvae_loss_function(recon_batch, x, mu, log_var, epoch)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test_rnnvae(epoch, loader):\n",
        "  test_load = loader\n",
        "  rnnvae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      l = l.to('cpu')\n",
        "      pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "      recon_batch, mu, log_var = rnnvae(x,c)\n",
        "     ## loss = F.mse_loss(x.flatten(), recon_batch.flatten(), reduction = 'sum')\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sov2kEXxGCE"
      },
      "source": [
        "##hyperparam, build, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwyap361ODY-"
      },
      "source": [
        "batch_size_vae =  200\n",
        "vae_epochs = 300\n",
        "latent_size = 16\n",
        "cycles = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89eDJA7ODY_"
      },
      "source": [
        "# build model\n",
        "\n",
        "rnnvae = RNN_VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, embedsize= 4, num_classes=num_classes, hsize=150, num_layers=1).double()\n",
        "rnnvae = rnnvae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(rnnvae.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgP82nmBODZB"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0xCWPAkODZD"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_rnn_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_rnnvae(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFrmZLMxMB0"
      },
      "source": [
        "##EVAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVoWWgY9toLd"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(train_index)), 10)\n",
        "\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "ids = []\n",
        "scan_similarity = []\n",
        "input_similarity = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "\n",
        "for iii, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  seq,_ = process_generated(x1)\n",
        "  #print(y)\n",
        "  latent_space = rnnvae.encode(x,y)\n",
        "  print('\\n', latent_space)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "  yi = int(np.asarray(y.to('cpu')).argmax(1))\n",
        "  yid = l_di[yi]\n",
        "  if iii <10:\n",
        "    print('\\n >{} input \\n'.format(yid), seq) \n",
        "  \n",
        "  for ii in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    if iii<10:\n",
        "      print('>{} generated: {} \\n'.format(yid, ii+1), gen_seq)\n",
        "    else:\n",
        "      pass\n",
        "    seqsplit = split(seq)\n",
        "    gensplit = split(gen_seq)\n",
        "    tally = 0\n",
        "    for i in range(len(seqsplit)):\n",
        "      a= seqsplit[i]\n",
        "      if i<len(gensplit):\n",
        "        b= gensplit[i]\n",
        "        if a==b:\n",
        "          tally+=1\n",
        "        else:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "    sim = tally/len(seqsplit)\n",
        "    mostsim = find_similar(gen_seq)\n",
        "    input_similarity.append(sim)\n",
        "    scan_similarity.append(mostsim)\n",
        "    if iii<10:\n",
        "      print('% similarity', sim)\n",
        "      print('most similar in training set:',mostsim)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print('\\n \\n mean input similarity', mean(input_similarity))\n",
        "print('mean scanned similarity', mean(scan_similarity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJAqz4t-7fmL"
      },
      "source": [
        "glycine1 = torch.tensor([[ 0.8636, -0.2653, -0.0748,  1.3591,  0.9868, -0.8168,  2.1340,  0.4987,\n",
        "          0.1397, -1.0721,  0.6553,  0.0429, -0.4242, -0.1992, -0.1195, -1.1274]])\n",
        "\n",
        "glycine2 = torch.tensor([[-0.9762, -0.8972, -1.9723,  2.8120,  1.2544, -0.4263,  1.0611,  0.4093,\n",
        "          1.6840,  2.6942, -1.3499,  0.3113, -1.5799,  0.4376, -1.8718,  0.3846]])\n",
        "\n",
        "dif = glycine2-glycine1\n",
        "\n",
        "steps = 5\n",
        "\n",
        "step = dif/steps\n",
        "yy = torch.tensor([[0,1,0]]).to(device).double()\n",
        "for i in range(steps+1):\n",
        "  latent_step = glycine1 + i*step\n",
        "  latent_step = latent_step.to(device).double()\n",
        "  gen = rnnvae.generate_similar(latent_step, yy)\n",
        "  gen_seq, gen_code = process_generated(gen=gen)\n",
        "  print(gen_seq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG5iEfd91gM7"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=l1_total, s=10)\n",
        "ax.set_title('Clustering by length')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrafWSBpwYLP"
      },
      "source": [
        "#compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C88rEHQPw-K5"
      },
      "source": [
        "from Bio.Align import substitution_matrices\n",
        "from Bio import pairwise2\n",
        "\n",
        "blosum62 = substitution_matrices.load('BLOSUM62')\n",
        "aligner = Align.PairwiseAligner()\n",
        "aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
        "aligner.mode = 'global'\n",
        "\n",
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print('\\n >KF99 UM2020 \\n', a)\n",
        "  latent_space_rnn = rnnvae.encode(x,y)\n",
        "  latent_space_vae = vae.encode(x1)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space_rnn.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled_rnn = noise+latent_space_rnn\n",
        "    sampled_vae = noise+latent_space_vae\n",
        "    gen_rnn = rnnvae.generate_similar(sampled_rnn, y)\n",
        "    gen_vae = vae.generate_similar(sampled_vae, y)\n",
        "    gen_rnn, gen_code = process_generated(gen=gen_rnn)\n",
        "    gen_vae, gen_code1 = process_generated(gen=gen_vae)\n",
        "    rnn_score = aligner.align(a, gen_rnn)\n",
        "    rnn_score = rnn_score.score\n",
        "    vae_score = aligner.align(a, gen_vae)\n",
        "    vae_score = vae_score.score\n",
        "    print('>KF99 RNN \\n', gen_rnn)\n",
        "    print('>KF99 MLP \\n', gen_vae)\n",
        "    print('RNN score: ', rnn_score)\n",
        "    print('MLP score: ', vae_score)\n",
        "    print('% improvement: ', 100*(rnn_score-vae_score)/vae_score)\n",
        "\n",
        "      \n",
        "    alignment = pairwise2.align.localds(a, gen_rnn, blosum62, -10, -1)\n",
        "    ga1 = pairwise2.align.globalxx(a, gen_rnn)\n",
        "    print( 'ga rnn: ', ga1[0].score)\n",
        "    print(alignment[0].score)\n",
        "\n",
        "    alignment2 = pairwise2.align.localds(a, gen_vae, blosum62, -10, -1)\n",
        "    ga2 = pairwise2.align.globalxx(a, gen_vae)\n",
        "    print('ga mlp: ', ga2[0].score)\n",
        "    print(alignment2[0].score)\n",
        "\n",
        "print(latent_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF1bVN7rLL-Z"
      },
      "source": [
        "\n",
        "blosum62 = substitution_matrices.load('BLOSUM62')\n",
        "aligner = Align.PairwiseAligner()\n",
        "aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
        "aligner.mode = 'global'\n",
        "\n",
        "score1rnn = []\n",
        "score1mlp = []\n",
        "score2rnn = []\n",
        "score2mlp = []\n",
        "score3rnn = []\n",
        "score3mlp = []\n",
        "\n",
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 30)\n",
        "rnnvae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  print(i/len(rnnvae_test_loader)*100, '%')\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  latent_space_rnn = rnnvae.encode(x,y)\n",
        "  latent_space_vae = vae.encode(x1)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space_rnn.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled_rnn = noise+latent_space_rnn\n",
        "    sampled_vae = noise+latent_space_vae\n",
        "    gen_rnn = rnnvae.generate_similar(sampled_rnn, y)\n",
        "    gen_vae = vae.generate_similar(sampled_vae, y)\n",
        "    gen_rnn, _ = process_generated(gen=gen_rnn)\n",
        "    gen_vae, _ = process_generated(gen=gen_vae)\n",
        "\n",
        "    rnn_score = aligner.align(a, gen_rnn)\n",
        "    rnn_score = rnn_score.score\n",
        "    score1rnn.append(rnn_score)\n",
        "    vae_score = aligner.align(a, gen_vae)\n",
        "    vae_score = vae_score.score\n",
        "    score1mlp.append(vae_score)\n",
        "\n",
        "\n",
        "\n",
        "    rnn2 = pairwise2.align.localds(a, gen_rnn, blosum62, -10, -1)\n",
        "    score2rnn.append(rnn2[0].score)\n",
        "    mlp2 = pairwise2.align.localds(a, gen_vae, blosum62, -10, -1)\n",
        "    score2mlp.append(mlp2[0].score)\n",
        "\n",
        "\n",
        "    rnn3 =  pairwise2.align.globalxx(a, gen_rnn)\n",
        "    score3rnn.append(rnn3[0].score)\n",
        "    mlp3 =  pairwise2.align.globalxx(a, gen_vae)\n",
        "    score3mlp.append(mlp3[0].score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQpBcGhSh1uG"
      },
      "source": [
        "print(mean(score1rnn), mean(score1mlp))\n",
        "print(mean(score2rnn), mean(score2mlp))\n",
        "print(mean(score3rnn), mean(score3mlp))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sluwWRNwljON"
      },
      "source": [
        "\n",
        "a,b = np.asarray(score1rnn), np.asarray(score1mlp)\n",
        "dif1 = a-b\n",
        "a,b = np.asarray(score2rnn), np.asarray(score2mlp)\n",
        "dif2 = a-b\n",
        "a,b = np.asarray(score3rnn), np.asarray(score3mlp)\n",
        "dif3 = a-b\n",
        "\n",
        "print(np.mean(dif2), np.std(dif2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkaf0pfMxUso"
      },
      "source": [
        "#Non-functional architectures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0edb2vY2bd4"
      },
      "source": [
        "## RNN-MLP vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT2dQrpf2beB"
      },
      "source": [
        "https://github.com/lyeoni/pytorch-mnist-VAE/blob/master/pytorch-mnist-VAE.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVmZmC3V2beC"
      },
      "source": [
        " class VAE(nn.Module):\n",
        "    def __init__(self, in_dims=1, hid1_dims=1, hid2_dims=8, num_classes=num_classes, negative_slope=0.1):\n",
        "        super(VAE, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.hid1_dims = hid1_dims\n",
        "        self.hid2_dims = hid2_dims\n",
        "        self.num_classes = num_classes\n",
        "        self.negative_slope = negative_slope\n",
        "        \n",
        "        self.gru = nn.GRU(input_size=4, hidden_size= 128, num_layers= 1, bidirectional= True, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dims, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Linear(128+256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, hid1_dims)\n",
        "        self.fc_var = nn.Linear(128, hid1_dims)\n",
        "\n",
        "        # Conditioner\n",
        "        self.conditioner = nn.Sequential(\n",
        "            nn.Linear(num_classes, 16),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(16,32),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(32, hid2_dims),\n",
        "            nn.ReLU() #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hid1_dims, 128), \n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, 512), \n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(512, in_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        _, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0).contiguous()\n",
        "        h1 = torch.flatten(h, start_dim=1, end_dim = 2).contiguous()\n",
        "        # Encode input\n",
        "        # Encode input\n",
        "        x = x.reshape(x.size(0), 4*x.size(1))\n",
        "        h2 = self.encoder(x)\n",
        "        h = torch.cat((h1,h2), dim=1)\n",
        "        h = self.combine(h)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        # Encode label\n",
        "        \n",
        "        hy = self.conditioner(y)\n",
        "        # Hidden representation\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        #print('hx:', hx.shape, 'hy:', hy.shape, 'y:', y.shape)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        # Decode\n",
        "        y_ = self.decoder(hx)\n",
        "        return y_, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "        hy = self.conditioner(y)\n",
        "        hx = self._sample(y.shape[0]).type_as(hy)\n",
        "        h = torch.cat([hx, hy], dim=1)\n",
        "        h = torch.cat([hx,y], dim = 1)\n",
        "        y = self.decoder(hx)\n",
        "        return y\n",
        "    def generate_similar(self,l,y):\n",
        "      h = torch.cat([l,y], dim=1)\n",
        "      z = self.decoder(l)\n",
        "      return(z)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      l1 = self.encoder(x)\n",
        "      mu, logvar = self.fc_mu(l1), self.fc_var(l1)\n",
        "      l2 = self._reparameterize(mu, logvar)\n",
        "      return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_var(h)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.hid1_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EwrbDpY2beC"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def vae_loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) \n",
        "    return BCE + 1.0*KLD #see Serena Yeung et. al. for KL scaling factor explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2clXWam2beC"
      },
      "source": [
        "def train_vae(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  vae.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = vae_loss_function(recon_batch, x, mu, log_var)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(vae.parameters(), 4)\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t63CtONl2beC"
      },
      "source": [
        "def test_vae(epoch, loader):\n",
        "  test_load = loader\n",
        "  vae.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c, l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = vae(x,c)\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IApO1Ubo2beD"
      },
      "source": [
        "batch_size_vae = 100\n",
        "vae_epochs = 150\n",
        "latent_size = 16\n",
        "conditional_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpqnwJBM2beD"
      },
      "source": [
        "# build model\n",
        "vae = VAE(in_dims=4*(padded_sequence.size(0)), hid1_dims = latent_size, hid2_dims = conditional_size, num_classes=num_classes, negative_slope=.01).double()\n",
        "vae = vae.to(device)\n",
        "optimizer_vae = torch.optim.Adam(vae.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4YjL2P2beD"
      },
      "source": [
        "#rng = np.random.default_rng()\n",
        "#rng.shuffle(vae_index)\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msVEbbeR2beD"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_vae(epoch, optimizer_vae, vae_train_loader)\n",
        "    test_vae(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2lHqtb52beE"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "ys = []\n",
        "ls = []\n",
        "l1 = []\n",
        "vae_test_loader = data.DataLoader(dataset(train_x[:,test_index,:], train_y[test_index,:], train_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(vae_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  #print(y)\n",
        "  latent_space = vae.encode(x1)\n",
        "  ls.append(latent_space.to('cpu').detach().numpy())\n",
        "  ys.append(y.to('cpu').detach().numpy())\n",
        "  l1.append(l.to('cpu').detach().numpy())\n",
        "\n",
        "  if i <20:\n",
        "    print('\\n >KF99 UM2020 \\n', a) \n",
        "    for i in range(number_samples):\n",
        "      noise = torch.randn((latent_space.shape))*variability\n",
        "      noise = noise.double().to(device)\n",
        "      sampled = noise+latent_space\n",
        "      gen = vae.generate_similar(sampled, y)\n",
        "      gen_seq, gen_code = process_generated(gen=gen)\n",
        "      print('>KF99 UM202{} \\n'.format(i+1), gen_seq )\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(latent_space)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLiO70z2beE"
      },
      "source": [
        "latent_space_total =ls[0]\n",
        "for i in range(1,len(ls)):\n",
        "  l = ls[i]\n",
        "  latent_space_total = np.concatenate((latent_space_total,l), axis=0)\n",
        "\n",
        "print(latent_space_total.shape)\n",
        "\n",
        "ys_total =ys[0]\n",
        "for i in range(1,len(ys)):\n",
        "  y = ys[i]\n",
        "  ys_total = np.concatenate((ys_total,y), axis=0)\n",
        "ys_total = ys_total.argmax(1).reshape(-1,1)\n",
        "\n",
        "l1_total = l1[0]\n",
        "for i in range(1, len(l1)):\n",
        "  l = l1[i]\n",
        "  l1_total = np.concatenate((l1_total, l), axis=0)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(latent_space_total)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(sets)):\n",
        "  ids.append(i)\n",
        "\n",
        "l_di = dict(zip(ids, sets))\n",
        "\n",
        "ribo_labels = pd.DataFrame(ys_total)\n",
        "ribo_labels = ribo_labels.replace(l_di)\n",
        "\n",
        "pcs = pd.DataFrame(pcs)\n",
        "\n",
        "pcs_data = pd.concat((pcs, ribo_labels), axis=1)\n",
        "pcs_data.columns = ['x', 'y', 'label']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pcs_data['x'], pcs_data['y'], c=l1_total, s=10)\n",
        "ax.set_title('Clustering by length')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "facet = sns.lmplot(data=pcs_data, x='x', y='y', hue='label', \n",
        "                   fit_reg=False, legend=True, legend_out=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIYlZlVwxXf"
      },
      "source": [
        "##RNNVAE w RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnC9kiKpw3nU"
      },
      "source": [
        " class RNN_VAERNN(nn.Module):\n",
        "    def __init__(self, latent_dims=128, num_encoder_layers = 2, encoder_hidden_size = 128 , num_decoder_layers=2, decoder_hidden_size = 128, seqlen = train_x.shape[0], expansion_size= 5):\n",
        "        super(RNN_VAERNN, self).__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.decoder_hidden_size = decoder_hidden_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.encoder_hidden_size = encoder_hidden_size\n",
        "        self.seqlen = seqlen\n",
        "        self.expansion_size= expansion_size\n",
        "        \n",
        "        self.gru1 = nn.GRU(input_size=4, hidden_size= self.encoder_hidden_size, num_layers= num_encoder_layers, bidirectional= True, dropout=0, batch_first = True)\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.encoder_hidden_size*self.num_encoder_layers, 256),\n",
        "            nn.LeakyReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(256,128),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(128, latent_dims)\n",
        "        self.fc_var = nn.Linear(128, latent_dims)\n",
        "        \n",
        "        self.latent_to_seq= nn.Sequential(\n",
        "            nn.Linear(self.latent_dims, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, self.expansion_size*self.seqlen),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.latent_to_hidden = nn.Sequential(\n",
        "            nn.Linear(self.latent_dims, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(128, self.decoder_hidden_size*self.num_decoder_layers*2)\n",
        "        )\n",
        "        self.hidden_to_output = nn.Sequential(\n",
        "            nn.Linear(self.decoder_hidden_size*2, 256), \n",
        "            nn.LeakyReLU(),\n",
        "            #nn.Linear(256, 128),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(256, 4),\n",
        "            nn.Sigmoid())\n",
        "    \n",
        "\n",
        "        self.decoder = nn.GRU(self.expansion_size, self.decoder_hidden_size, self.num_decoder_layers, bidirectional=True, batch_first = True)\n",
        "       \n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \n",
        "        gru_out, h = self.gru1(x)\n",
        "        hp = h.permute(1,2,0)\n",
        "        hf = torch.flatten(hp, start_dim=1, end_dim = 2)\n",
        "        hx = self.encoder(hf)\n",
        "        mu, logvar = self.fc_mu(hx), self.fc_var(hx)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        lin_seq = self.latent_to_seq(hx)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(hx)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid) ###, h)\n",
        "        y = self.hidden_to_output(decoder_output)\n",
        "        return y, mu, logvar\n",
        "\n",
        "    def generate(self, y):\n",
        "\n",
        "        lin_seq = self.latent_to_seq(y)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(y)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid)\n",
        "        y = self.hidden_to_output(decoder_output)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def generate_similar(self,l,y):\n",
        "\n",
        "        lin_seq = self.latent_to_seq(l)\n",
        "        lin_seq = torch.reshape(lin_seq, (lin_seq.size(0), int(lin_seq.size(1)/self.expansion_size), self.expansion_size)).contiguous()\n",
        "        lin_seq = lin_seq.clone()\n",
        "        hid = self.latent_to_hidden(l)\n",
        "        hid = torch.reshape(hid, [hid.size(0), 2*self.num_decoder_layers, self.decoder_hidden_size]).contiguous()\n",
        "        hid = hid.permute(1,0,2).contiguous()\n",
        "        decoder_output, _ = self.decoder(lin_seq, hid)\n",
        "        z = self.hidden_to_output(decoder_output)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru1(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        l2 = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return l2\n",
        "\n",
        "    def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.latent_dims).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAj5162p2dY1"
      },
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def rnnvae2_loss_function(recon_x, x, mu, log_var, epo):\n",
        "    BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) #*np.sqrt(epo/vae_epochs)\n",
        "    return BCE + 0*KLD #see Serena Yeung et. al. for KL scaling factor explanation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNrlnajw2fgS"
      },
      "source": [
        "def train_rnn_vae2(epoch, opt, loader):\n",
        "  train_loader = loader\n",
        "  rnnvae2.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(train_loader):\n",
        "      opt.zero_grad()\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      x = x.double().to(device)\n",
        "      recon_batch, mu, log_var = rnnvae2(x,c)\n",
        "      #print(recon_batch.shape, x.shape)\n",
        "      #recon_batch = recon_batch.permute(1,0,2).double().to(device)\n",
        "      loss = rnnvae2_loss_function(recon_batch, x, mu, log_var, epoch)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      opt.step()\n",
        "        \n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test_rnnvae2(epoch, loader):\n",
        "  test_load = loader\n",
        "  rnnvae2.eval()\n",
        "  test_loss = 0\n",
        "  for batch_idx, (x,c,l) in enumerate(test_load):\n",
        "      #x = x.reshape(x.size(0), 4*x.size(1))\n",
        "      recon_batch, mu, log_var = rnnvae2(x,c)\n",
        "\n",
        "      loss = F.binary_cross_entropy(recon_batch.flatten(), x.flatten(), reduction='sum')\n",
        "      test_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "  print('====> test loss: {:.4f}'.format(test_loss / len(test_load.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRVTFmuE5GKG"
      },
      "source": [
        "batch_size_vae = 200\n",
        "vae_epochs = 100\n",
        "train_index = range(len(train_x[0,:,0]))\n",
        "#train_index = random.sample(range(train_x.shape[1]), 200)\n",
        "test_index = random.sample(range(len(valid_index)), 100)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vokDPjYl2l6f"
      },
      "source": [
        "# build model\n",
        "latent_size = 16\n",
        "encode_layers = 1\n",
        "encode_hsize = 128\n",
        "decode_layers = 1\n",
        "decode_hsize = 128\n",
        "expansion_size = 5\n",
        "\n",
        "rnnvae2 = RNN_VAERNN(latent_dims = latent_size, num_encoder_layers=encode_layers, encoder_hidden_size=encode_hsize, num_decoder_layers=decode_layers, decoder_hidden_size=decode_hsize, seqlen=train_x.shape[0], expansion_size=expansion_size).double()\n",
        "rnnvae2 = rnnvae2.to(device)\n",
        "optimizer_vae2 = torch.optim.Adam(rnnvae2.parameters(), lr = .001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc2nk1052oRa"
      },
      "source": [
        "for epoch in range(vae_epochs):\n",
        "    train_rnn_vae2(epoch, optimizer_vae2, vae_train_loader)\n",
        "    test_rnnvae2(epoch, vae_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH22f-Bun_BT"
      },
      "source": [
        "#without ReLU in pseudoseq\n",
        "#train = 173\n",
        "#test  = 172"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlzxjsYYqTDz"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae2_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae2_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print(y)\n",
        "  print(a)\n",
        "  latent_space = rnnvae2.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate(sampled)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print(gen_seq)\n",
        "\n",
        "print(latent_space)\n",
        "print(gen[0,:10,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inco12Bmlq_D"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae_test_loader = data.DataLoader(dataset_rnn(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = 1)\n",
        "for i, (x,y,l) in enumerate(rnnvae_test_loader):\n",
        "  l = l.to('cpu')\n",
        "  x = x.double().to(device)\n",
        "  pack = torch.nn.utils.rnn.pack_padded_sequence(x, l , batch_first=True, enforce_sorted=False)\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print('\\n >KF99 UM2020 \\n', a)\n",
        "  latent_space = rnnvae2.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print('>KF99 UM2021 \\n', gen_seq)\n",
        "\n",
        "print(latent_space)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbhZFjXtiWQq"
      },
      "source": [
        "##seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxrs9mG46kw1"
      },
      "source": [
        "def seq2seq_loss(recon_x, x, mu, log_var, epo):\n",
        "  BCE = F.binary_cross_entropy(recon_x.flatten(), x.flatten(), reduction='sum')\n",
        "  KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())*np.sqrt(epo/seq2seq_epochs)\n",
        "  return BCE + 1*KLD #see Serena Yeung et. al. for KL scaling factor explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wzvgn6C-XCS"
      },
      "source": [
        " class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size=64, latent_size=128, num_layers = 1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size=4, hidden_size= self.hidden_size, num_layers= self.num_layers, bidirectional= True, dropout=0, batch_first = True)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2*self.hidden_size*self.num_layers, 128),\n",
        "            nn.ReLU(), #nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.fc_mu = nn.Linear(128, latent_size)\n",
        "        self.fc_var = nn.Linear(128, latent_size)\n",
        "\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "\n",
        "        return hx, mu, logvar\n",
        "\n",
        "\n",
        "    def _represent(self, x):\n",
        "         ##self.hidden = self.init_hidden(x.size(0)).to(device)\n",
        "        gru_out, h = self.gru(x) ##, self.hidden)\n",
        "        h = h.permute(1,2,0)\n",
        "        h = torch.flatten(h, start_dim=1, end_dim = 2)\n",
        "        # Encode input\n",
        "        h2 = self.encoder(h)\n",
        "        mu, logvar = self.fc_mu(h2), self.fc_var(h2)\n",
        "        hx = self._reparameterize(mu, logvar)\n",
        "        return hx\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).type_as(mu)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "\n",
        "\n",
        "    def _sample(self, num_samples):\n",
        "        return torch.FloatTensor(num_samples, self.latent_size).normal_()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdCWP1Lw474O"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, layers, output_size = 4):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "        self.gru = nn.GRU(4, hidden_size, num_layers=self.layers, bidirectional = False, batch_first=True)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size),\n",
        "            nn.Sigmoid())\n",
        "          \n",
        "        self.softmax = nn.Softmax(dim=2) #sub for softmax?\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "\n",
        "        output = self.out(output) #output[0]? \n",
        "      \n",
        "        return output, hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lhQ_AntLQVb"
      },
      "source": [
        "dec = DecoderRNN(hidden_size=128, layers=2)\n",
        "h = torch.randn([2,100,128])\n",
        "input = torch.randn([100,19,4])\n",
        "i = input[:,0,:]\n",
        "i = i.unsqueeze(1)\n",
        "s = torch.zeros_like(input)\n",
        "for ij in range(19):\n",
        "\n",
        "  out, h = dec(i,h)\n",
        "  s[:,ij,:] = out[:,-1,:]\n",
        "  i = input[:,:ij+1,:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtllK1oxpBXZ"
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, decode_layers, lsize, hsize):\n",
        "    super(Seq2seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.latent_to_hidden = nn.Linear(lsize , hsize)\n",
        "\n",
        "  def forward(self, input, teacher_forcing):\n",
        "\n",
        "    latent, mu, log_var = self.encoder(input)\n",
        "\n",
        "    new_hidden = self.latent_to_hidden(latent)\n",
        "    #new_hidden = latent\n",
        "\n",
        "\n",
        "    tensor_length = input.size(1)\n",
        "\n",
        "    outputs = torch.zeros_like(input)\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing else False\n",
        "\n",
        "    ##decoder_input = input[:,0,:]\n",
        "    ##decoder_input = torch.unsqueeze(decoder_input, dim=1)\n",
        "    decoder_input = torch.zeros([input.size(0), 1, input.size(2)], requires_grad=True).double().to(device)\n",
        "\n",
        "    decoder_hidden = torch.stack([new_hidden for _ in range(decode_layers)], dim=0)\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(tensor_length):\n",
        "            #print('decode input:', decoder_input.shape)\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "           \n",
        "            outputs[:,di,:] = decoder_output[:,-1,:]\n",
        "            decoder_input = input[:,:di+1,:].clone()\n",
        "          \n",
        "\n",
        "            ##decoder_input = input[:,di,:]  # Teacher forcing\n",
        "            ##decoder_input = torch.unsqueeze(decoder_input, dim=1)\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(tensor_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            outputs[:,di,:] = decoder_output[:,-1,:]\n",
        "            decoder_input = outputs[:,:di+1,:].clone()\n",
        "            #decoder_input = decoder_input.detach()  # detach from history as input\n",
        "\n",
        "\n",
        "    return outputs, latent, mu, log_var\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAEMlSL_x7ly"
      },
      "source": [
        "a = torch.tensor([[3,3,3], [2,2,2]] )\n",
        "a[-1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg6dHqPRr29Q"
      },
      "source": [
        "def train_seq2seq1(model, loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "\n",
        "  for i, (x,y,l) in enumerate(loader):\n",
        "    optimizer.zero_grad()\n",
        "    x = x.double().to(device)\n",
        "    reconstructed, latent, mu, log_var = model(x, 1-.5*np.sqrt(epoch/seq2seq_epochs))\n",
        "    loss = seq2seq_loss(reconstructed, x, mu, log_var, epoch)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "\n",
        "  print('\\n ====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(loader.dataset)))\n",
        "  return reconstructed, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvQq3L0xxF1"
      },
      "source": [
        "def evaluate(model, loader, epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (x,y,l) in enumerate(loader):\n",
        "            x = x.double().to(device)\n",
        "\n",
        "            output, latent, mu, logvar = model(x, 0) #turn off teacher forcing\n",
        "\n",
        "            loss = seq2seq_loss(output, x, mu, logvar, epo=0)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        print(' ====> Average test loss:     {:.4f}'.format( epoch_loss / len(loader.dataset)))\n",
        "    return output, x, epoch_loss / len(loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aICt2H7bxR1z"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4MqeY8Gys93"
      },
      "source": [
        "a = torch.randn([3,5])\n",
        "b = torch.zeros_like(a)\n",
        "for i in range(a.size(0)):\n",
        "  b[i,:] = a[i,:]\n",
        "\n",
        "print(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjMDHy6JJbks"
      },
      "source": [
        "batch_size_vae = 4\n",
        "seq2seq_epochs = 50\n",
        "#train_index = range(len(train_x[0,:,0]))\n",
        "train_index = random.sample(range(train_x.shape[1]), 200)\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "vae_train_loader = data.DataLoader(dataset(train_x[:,train_index,:], train_y[train_index,:], train_length[train_index]), batch_size = batch_size_vae)\n",
        "vae_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:], test_length[test_index]), batch_size = batch_size_vae)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICLxFO_0rmkR"
      },
      "source": [
        "latent_size = 3\n",
        "encode_hidden_size = 128\n",
        "decode_hidden_size = 256\n",
        "decode_layers = 2\n",
        "encoder = EncoderRNN(hidden_size = encode_hidden_size, latent_size=latent_size, num_layers = 2).double().to(device)\n",
        "decoder = DecoderRNN(hidden_size=decode_hidden_size, layers = decode_layers).double().to(device)\n",
        "seq2seq = Seq2seq(encoder=encoder, decoder = decoder, decode_layers=decode_layers, lsize = latent_size, hsize = decode_hidden_size).double().to(device)\n",
        "seq2seq_optimizer = torch.optim.Adam(seq2seq.parameters(), lr=.001)\n",
        "seq2seq.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9oO63mtKfw"
      },
      "source": [
        "with torch.autograd.set_detect_anomaly(True):\n",
        "  for epoch in range(seq2seq_epochs):\n",
        "      out = train_seq2seq1(seq2seq, vae_train_loader, seq2seq_optimizer, epoch)\n",
        "      _,__,test_loss = evaluate(seq2seq, vae_test_loader, epoch)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tsJU62UQvRE"
      },
      "source": [
        "pred = out[0]\n",
        "tar = out[1]\n",
        "print(pred[0, :10,:])\n",
        "print(tar[0, :10  ,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_WjZr5pj43G"
      },
      "source": [
        "\n",
        "gen_seq, gen_code = process_generated(gen=pred[1])\n",
        "seq, _ = process_generated(gen=tar[1])\n",
        "print(gen_seq)\n",
        "print(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEbCgDjgjysc"
      },
      "source": [
        "variability = 0.0\n",
        "number_samples = 1\n",
        "test_index = random.sample(range(len(valid_index)), 10)\n",
        "rnnvae2_test_loader = data.DataLoader(dataset(test_x[:,test_index,:], test_y[test_index,:]), batch_size = 1)\n",
        "for i, (x,y) in enumerate(rnnvae2_test_loader):\n",
        "  x1 = x.reshape(x.size(0), 4*x.size(1))\n",
        "  a,_ = process_generated(x1)\n",
        "  print(y)\n",
        "  print(a)\n",
        "  latent_space = EncoderRNN.encode(x)\n",
        "  \n",
        "  for i in range(number_samples):\n",
        "    noise = torch.randn((latent_space.shape))*variability\n",
        "    noise = noise.double().to(device)\n",
        "    sampled = noise+latent_space\n",
        "    gen = rnnvae2.generate_similar(sampled, y)\n",
        "    gen_seq, gen_code = process_generated(gen=gen)\n",
        "    print(gen_seq)\n",
        "\n",
        "print(latent_space)\n",
        "print(gen[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mTN7h-8Pzkk"
      },
      "source": [
        "enc = EncoderRNN(hid1_dims=128)\n",
        "enc._init_weights()\n",
        "a = torch.randn([10, 110, 4])\n",
        "out, mu, log = enc(a)\n",
        "dec = DecoderRNN(hidden_size=128)\n",
        "hid = torch.unsqueeze(out, dim=0)\n",
        "ran = torch.randn([10,1,4])\n",
        "loss = 0\n",
        "criter = nn.MSELoss()\n",
        "for i in range(30):\n",
        "  outp, hid = dec(ran, hid)\n",
        "  if i == 0:\n",
        "    print(outp)\n",
        "  else:\n",
        "    ran = outp.detach()\n",
        "    tar = a[:,i,:]\n",
        "    loss += criter(tar.flatten(), ran.flatten())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4p39Rh9rn0E"
      },
      "source": [
        "#1. set up for loop to create many synthetic sequences\n",
        "#2. convert synthetic sequences to binary and append to real sequences\n",
        "#3. create new train_loader to incorporate new data\n",
        "#4. train new RNN on new data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMfPPvsYUtMZ"
      },
      "source": [
        "\n",
        "generated_list = []\n",
        "generated_id = []\n",
        "synthetic = []\n",
        "for ij in range(num_classes):\n",
        "  print('NEW TYPE')\n",
        "  gen_type = ij+1\n",
        "  a = np.array(gen_type).reshape(-1,1)\n",
        "  b = enc.transform(a).toarray()\n",
        "  c = torch.tensor(b).double().to(device)\n",
        "  avg_len = []\n",
        "  for ii in range(number_samples):\n",
        "    sample = model.generate(c)\n",
        "\n",
        "    test = sample.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "    test = np.asarray(test.detach())\n",
        "\n",
        "    pred_binary = np.zeros_like(test)\n",
        "    pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "    a = np.sum(test, 1)\n",
        "\n",
        "    #plt.plot(a)\n",
        "    #plt.show()\n",
        "\n",
        "    stop_point = []\n",
        "    for i, j in enumerate(a):\n",
        "      if j <=.1:\n",
        "        stop_point.append(i)\n",
        "\n",
        "    #print(stop_point[0:4])\n",
        "    if not stop_point:\n",
        "      pass\n",
        "    elif stop_point[0] <= 40:\n",
        "      pass\n",
        "    else:\n",
        "      pred_binary = pred_binary[:stop_point[0],:]\n",
        "      avg_len.append(stop_point[0])\n",
        "\n",
        "      s = []\n",
        "      for i in pred_binary:\n",
        "        if i[0] == 1:\n",
        "          s.append('A')\n",
        "        elif i[1] == 1:\n",
        "          s.append('C')\n",
        "        elif i[2] == 1:\n",
        "          s.append('G')\n",
        "        elif i[3] == 1:\n",
        "          s.append('T')\n",
        "\n",
        "      result = ''\n",
        "      for element in s:\n",
        "          result += str(element)\n",
        "\n",
        "      generated = [result]\n",
        "\n",
        "      print(generated)\n",
        "      synthetic.append(generated)\n",
        "      seq, _ = sequence_one_hot(generated)\n",
        "      #print(seq.size(0))\n",
        "      zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "      generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "      g = generated_sequence.permute(1,0,2)\n",
        "      g = g.to(device)\n",
        "      generated_list.append(g)\n",
        "\n",
        "      generated_id.append(c)\n",
        "\n",
        "  # print(len(generated_list))\n",
        "  # print(mean(avg_len))\n",
        "a = generated_list[0]\n",
        "for i in range(1,len(generated_list)):\n",
        "  a = torch.cat((a,generated_list[i]))\n",
        "\n",
        "b = a.permute(1,0,2)\n",
        "synthetic = b.double().to(device)\n",
        "\n",
        "train_mixed = torch.cat((train_x,synthetic), dim=1)\n",
        "\n",
        "#print(train_mixed.shape)\n",
        "\n",
        "a = generated_id[0]\n",
        "for i in range(1,len(generated_id)):\n",
        "  a = torch.cat((a, generated_id[i]))\n",
        "\n",
        "mixed_y = torch.cat((train_y, a))\n",
        "\n",
        "return train_mixed, mixed_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZsKqRMbw-TI"
      },
      "source": [
        "def generate_synthetic(number_samples, model):\n",
        "  generated_list = []\n",
        "  generated_id = []\n",
        "  synthetic = []\n",
        "  for ij in range(num_classes):\n",
        "    print('NEW TYPE')\n",
        "    gen_type = ij+1\n",
        "    a = np.array(gen_type).reshape(-1,1)\n",
        "    b = enc.transform(a).toarray()\n",
        "    c = torch.tensor(b).double().to(device)\n",
        "    avg_len = []\n",
        "    for ii in range(number_samples):\n",
        "      sample = model.generate(c)\n",
        "\n",
        "      test = sample.reshape(padded_sequence.size(0),4).to('cpu')\n",
        "      test = np.asarray(test.detach())\n",
        "\n",
        "      pred_binary = np.zeros_like(test)\n",
        "      pred_binary[np.arange(len(test)), test.argmax(1)] = 1\n",
        "\n",
        "      a = np.sum(test, 1)\n",
        "\n",
        "      #plt.plot(a)\n",
        "      #plt.show()\n",
        "\n",
        "      stop_point = []\n",
        "      for i, j in enumerate(a):\n",
        "        if j <=.1:\n",
        "          stop_point.append(i)\n",
        "\n",
        "      #print(stop_point[0:4])\n",
        "      if not stop_point:\n",
        "        pass\n",
        "      elif stop_point[0] <= 40:\n",
        "        pass\n",
        "      else:\n",
        "        pred_binary = pred_binary[:stop_point[0],:]\n",
        "        avg_len.append(stop_point[0])\n",
        "\n",
        "        s = []\n",
        "        for i in pred_binary:\n",
        "          if i[0] == 1:\n",
        "            s.append('A')\n",
        "          elif i[1] == 1:\n",
        "            s.append('C')\n",
        "          elif i[2] == 1:\n",
        "            s.append('G')\n",
        "          elif i[3] == 1:\n",
        "            s.append('T')\n",
        "\n",
        "        result = ''\n",
        "        for element in s:\n",
        "            result += str(element)\n",
        "\n",
        "        generated = [result]\n",
        "\n",
        "        print(generated)\n",
        "        synthetic.append(generated)\n",
        "        seq, _ = sequence_one_hot(generated)\n",
        "        #print(seq.size(0))\n",
        "        zeros = torch.zeros((padded_sequence.size(0)-seq.size(0),1,4))\n",
        "        generated_sequence = torch.cat((seq, zeros))\n",
        "\n",
        "\n",
        "        g = generated_sequence.permute(1,0,2)\n",
        "        g = g.to(device)\n",
        "        generated_list.append(g)\n",
        "\n",
        "        generated_id.append(c)\n",
        "\n",
        "   # print(len(generated_list))\n",
        "   # print(mean(avg_len))\n",
        "  a = generated_list[0]\n",
        "  for i in range(1,len(generated_list)):\n",
        "    a = torch.cat((a,generated_list[i]))\n",
        "\n",
        "  b = a.permute(1,0,2)\n",
        "  synthetic = b.double().to(device)\n",
        "\n",
        "  train_mixed = torch.cat((train_x,synthetic), dim=1)\n",
        "\n",
        "  #print(train_mixed.shape)\n",
        "\n",
        "  a = generated_id[0]\n",
        "  for i in range(1,len(generated_id)):\n",
        "    a = torch.cat((a, generated_id[i]))\n",
        "\n",
        "  mixed_y = torch.cat((train_y, a))\n",
        "\n",
        "  return train_mixed, mixed_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMwt-qh5DzlS"
      },
      "source": [
        "train_mixed, mixed_y = generate_synthetic(100, vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJX8nKNG6WT2"
      },
      "source": [
        "learn_rate_rnn = .001\n",
        "rnn_epochs = 100\n",
        "rnn_batch_size = 200\n",
        "rnn_dropout_rate = .3\n",
        "rnn_layers = 1\n",
        "rnn_width = 128\n",
        "dense_width = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kocmd_j6b0l"
      },
      "source": [
        "\n",
        "rnn1 = RNN(drop_p = rnn_dropout_rate, num_layers=rnn_layers, h_rnn = rnn_width, fc_layer = dense_width, batch_size=rnn_batch_size).to(device)\n",
        "rnn1 = rnn1.double()\n",
        "rnn_optimizer = torch.optim.Adam(rnn1.parameters(), lr=learn_rate_rnn)\n",
        "\n",
        "rnn_train_index = range(len(train_x[0,:,0]))\n",
        "rnn_test_index = range(len(test_x[0,:100,0]))\n",
        "print('new_fold')\n",
        "\n",
        "train_loader = data.DataLoader(dataset_rnn(train_x[:,rnn_train_index,:], train_y[rnn_train_index,:]), batch_size=rnn_batch_size)\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=rnn_batch_size)\n",
        "\n",
        "for epoch in range(rnn_epochs):\n",
        "  train_losses = train(device, rnn_optimizer, train_loader, rnn1)\n",
        "  test_loss, y_pred_test1, y_test1= test_rnn(device, test_loader, rnn1)\n",
        "  avg_train_loss = sum(train_losses)/len(train_losses)\n",
        "  print('epoch:', epoch, 'loss:', avg_train_loss, \"test:\", test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuIT57KDB3N"
      },
      "source": [
        "\n",
        "rnn2 = RNN2(drop_p = rnn_dropout_rate, num_layers=rnn_layers, h_rnn = rnn_width, fc_layer = dense_width, batch_size=rnn_batch_size).to(device)\n",
        "rnn2 = rnn2.double()\n",
        "rnn_optimizer2 = torch.optim.Adam(rnn2.parameters(), lr=learn_rate_rnn)\n",
        "\n",
        "rnn_train_index2 = range(len(train_x[0,:,0]))\n",
        "rnn_test_index = range(len(train_mixed[0,:100,0]))\n",
        "print('new_fold')\n",
        "\n",
        "train_loader = data.DataLoader(dataset_rnn(train_mixed[:,rnn_train_index2,:], mixed_y[rnn_train_index2,:]), batch_size=rnn_batch_size)\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=rnn_batch_size)\n",
        "\n",
        "for epoch in range(rnn_epochs):\n",
        "  train_losses = train(device, rnn_optimizer2, train_loader, rnn2)\n",
        "  test_loss, y_pred_test2, y_test2 = test_rnn(device, test_loader, rnn2) \n",
        "  avg_train_loss = sum(train_losses)/len(train_losses)\n",
        "  print('epoch:', epoch, 'loss:', avg_train_loss, \"test:\", test_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkcI5GOEk4vP"
      },
      "source": [
        "rnn_test_index = range(test_x.shape[1]) #test_x.shape[1])\n",
        "test_loader = data.DataLoader(dataset_rnn(test_x[:,rnn_test_index,:], test_y[rnn_test_index,:]), batch_size=1000)\n",
        "_, y_pred_test2, y_test2= test_rnn(device, test_loader, rnn2) \n",
        "_, y_pred_test1, y_test1= test_rnn(device, test_loader, rnn1) \n",
        "\n",
        "\n",
        "p1, matrix1, avg_auc1, f11 = generate_report(y_pred_test1, y_test1)\n",
        "p1.show()\n",
        "print(avg_auc1)\n",
        "print(matrix1)\n",
        "p2, matrix2, avg_auc2, f12 = generate_report(y_pred_test2, y_test2)\n",
        "p2.show()\n",
        "print(avg_auc2)\n",
        "print(matrix2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}